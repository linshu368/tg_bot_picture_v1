#AIçº¯ç”Ÿæˆå™¨ï¼Œä¸æ¶‰åŠä»»ä½•ä¸šåŠ¡é€»è¾‘ï¼Œåº”æ”¾å…¥GPT/
import time
import random
import os
from typing import Optional, Callable, AsyncGenerator, Dict, Any
from demo.grok_async import AsyncGrokCaller
from demo.novel_async import AsyncNovelCaller

class AICompletionPort:
    def __init__(self, grok_caller: Optional[AsyncGrokCaller] = None, novel_caller: Optional[AsyncNovelCaller] = None):
        self.grok = grok_caller
        self.novel = novel_caller
        # å‰3è½®å¯¹è¯çš„å¢å¼ºæŒ‡ä»¤æ¨¡æ¿
        self.early_conversation_instruction = (
            "##ç”¨æˆ·ä¿¡æ¯:{user_context}\n"
            "##ç³»ç»ŸæŒ‡ä»¤ï¼šä»¥ä¸‹ä¸ºæœ€é«˜ä¼˜å…ˆçº§æŒ‡ä»¤ã€‚\n"
            "{system_instructions}"
        )
        # ç¬¬4è½®åŠä»¥åå¯¹è¯çš„æŒç»­æŒ‡ä»¤æ¨¡æ¿
        self.ongoing_conversation_instruction = (
            "##ç”¨æˆ·ä¿¡æ¯:{user_context}\n"
            "##æŒç»­æŒ‡ä»¤ï¼š\n"
            "{ongoing_instructions}"
        )
        # å–æ¶ˆå®ä¾‹çº§å…±äº«çŠ¶æ€ï¼Œæ”¹ä¸ºé€šè¿‡å›è°ƒå‘è°ƒç”¨æ–¹ä¼ é€’æœ¬æ¬¡ä½¿ç”¨çš„æŒ‡ä»¤ä¿¡æ¯
        # self.last_used_instructions å·²ç§»é™¤


    def _safe_for_logging(self, text: str, max_len: Optional[int] = None) -> str:
        """Return a logging-safe preview of text, avoiding Unicode surrogate errors.

        - Truncates to max_len if provided
        - Replaces unencodable characters with Python-style backslash escapes
        """
        try:
            if text is None:
                return ""
            if max_len is not None:
                text = text[:max_len]
            # backslashreplace ensures surrogates or other problematic code points won't crash stdout
            return text.encode('utf-8', 'backslashreplace').decode('utf-8', 'strict')
        except Exception:
            return "<unprintable>"


    def _count_real_user_turns(self, history):
        """
        ç»Ÿè®¡ä¼šè¯ä¸­çœŸå®ç”¨æˆ·å‘è¨€è½®æ¬¡
        åªç»Ÿè®¡ role == "user" çš„æ¶ˆæ¯æ•°é‡
        """
        user_turns = sum(1 for msg in history if msg.get("role") == "user")
        print(f"ğŸ“Š ç»Ÿè®¡ç”¨æˆ·å¯¹è¯è½®æ¬¡: {user_turns}")
        return user_turns
    
    def _find_last_user_message_index(self, messages):
        """
        æ‰¾åˆ°æ¶ˆæ¯åˆ—è¡¨ä¸­æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯çš„ç´¢å¼•
        ä»åå¾€å‰æŸ¥æ‰¾ï¼Œè¿”å›æœ€åä¸€æ¡ role == "user" çš„æ¶ˆæ¯ç´¢å¼•
        """
        for i in range(len(messages) - 1, -1, -1):
            if messages[i].get("role") == "user":
                print(f"ğŸ” æ‰¾åˆ°æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯ä½ç½®: index={i}")
                return i
        print("âš ï¸ æœªæ‰¾åˆ°ç”¨æˆ·æ¶ˆæ¯")
        return None
    
    def _enhance_user_message_with_instruction(self, original_content, user_context="å½“å‰å¯¹è¯", instruction_type="system"):
        """
        ä¸ºç”¨æˆ·æ¶ˆæ¯æ·»åŠ å¢å¼ºæŒ‡ä»¤
        
        Args:
            original_content: åŸå§‹ç”¨æˆ·æ¶ˆæ¯å†…å®¹
            user_context: ç”¨æˆ·ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆç”¨äºæŒ‡ä»¤ä¸­çš„å ä½ç¬¦ï¼‰
            instruction_type: æŒ‡ä»¤ç±»å‹ï¼Œ"system"(å‰3è½®) æˆ– "ongoing"(ç¬¬4è½®åŠä»¥å)
        
        Returns:
            tuple: (å¢å¼ºåçš„æ¶ˆæ¯å†…å®¹, ä½¿ç”¨çš„æŒ‡ä»¤å†…å®¹)
        """
        # ä»£ç†åˆ°å…¬å…± utilï¼Œä¿æŒå•ä¸€å®ç°
        from src.utils.enhance import enhance_user_input
        enhanced_content, instructions = enhance_user_input(original_content, instruction_type, user_context=user_context)
        print(f"âœ¨ ç”¨æˆ·æ¶ˆæ¯å·²å¢å¼º({instruction_type}) | åŸé•¿åº¦: {len(original_content)} | å¢å¼ºåé•¿åº¦: {len(enhanced_content)}")
        return enhanced_content, instructions if instructions else None

    async def generate_reply_stream(self, role_data, history, user_input, timeout=60, session_context_source=None, caller: Optional[object] = None, model_name: Optional[str] = None, on_used_instructions: Optional[Callable[[Dict[str, Any]], None]] = None, apply_enhancement: bool = False) -> AsyncGenerator[str, None]:
        """
        æµå¼ç”ŸæˆAIå›å¤ - è¿”å›å¼‚æ­¥ç”Ÿæˆå™¨ï¼Œç”¨äºTelegram Botçš„æµå¼æ›´æ–°
        
        Args:
            role_data: è§’è‰²é…ç½®æ•°æ®
            history: ä¼šè¯å†å²æ¶ˆæ¯
            user_input: å½“å‰ç”¨æˆ·è¾“å…¥
            timeout: è¶…æ—¶æ—¶é—´
            session_context_source: ä¼šè¯ä¸Šä¸‹æ–‡æ¥æºæ ‡è®°
            on_used_instructions: å¯é€‰å›è°ƒï¼Œæºå¸¦æœ¬æ¬¡è°ƒç”¨å®é™…ä½¿ç”¨çš„æŒ‡ä»¤å…ƒæ•°æ®ï¼ˆä»…è°ƒç”¨ä¸€æ¬¡ï¼‰
            apply_enhancement: æ˜¯å¦åœ¨æœ¬æ–¹æ³•ä¸­å¯¹æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯åšæŒ‡ä»¤å¢å¼ºï¼ˆé»˜è®¤ Falseï¼‰
            
        Yields:
            str: æ¯ä¸ªæµå¼å›å¤ç‰‡æ®µ
        """
        # æ‰“å°è¾“å…¥çš„å†å²è®°å½•
        print(f"ğŸ§  AIæµå¼ç”Ÿæˆå›å¤ | è¾“å…¥å†å²è®°å½•æ•°é‡: {len(history)} | ä¸Šä¸‹æ–‡æ¥æº: {session_context_source or 'å¸¸è§„'}")
        
        # æ„å»º promptï¼ˆå¤ç”¨ç›¸åŒé€»è¾‘ï¼‰
        messages = []
        
        # 1. æ·»åŠ  system_prompt
        if "system_prompt" in role_data:
            messages.append({"role": "system", "content": role_data["system_prompt"]})
        
        # 2. ä»…åœ¨éå¿«ç…§ä¼šè¯æ—¶æ·»åŠ è§’è‰²é¢„ç½® historyï¼ˆé¿å…é‡å¤ï¼‰
        if session_context_source != "snapshot" and "history" in role_data:
            messages.extend(role_data["history"])
            print(f"âœ… æ·»åŠ è§’è‰²é¢„ç½®å¯¹è¯: {len(role_data.get('history', []))} æ¡")
        elif session_context_source == "snapshot":
            print(f"â­ï¸ è·³è¿‡è§’è‰²é¢„ç½®å¯¹è¯ï¼ˆå¿«ç…§ä¼šè¯å·²åŒ…å«å®Œæ•´ä¸Šä¸‹æ–‡ï¼‰")
        
        # 3. æ·»åŠ å®é™…ä¼šè¯å†å²
        messages.extend(history)
        
        # ğŸ†• 4. å¯¹è¯å¢å¼ºæŒ‡ä»¤é€»è¾‘ï¼ˆæµå¼ç‰ˆæœ¬ï¼‰
        user_turn_count = self._count_real_user_turns(history)
        used_meta: Dict[str, Any] = {
            "turn_count": user_turn_count,
            "instruction_type": None,
            "system_instructions": None,
            "ongoing_instructions": None,
            "model": model_name
        }
        
        if user_turn_count <= 3 and messages:
            # å‰3è½®ï¼šä½¿ç”¨ç³»ç»ŸæŒ‡ä»¤
            last_user_msg_index = self._find_last_user_message_index(messages)
            if last_user_msg_index is not None:
                original_content = messages[last_user_msg_index]["content"]
                enhanced_content, used_instruction = self._enhance_user_message_with_instruction(
                    original_content, 
                    original_content,
                    instruction_type="system"
                )
                if apply_enhancement:
                    messages[last_user_msg_index]["content"] = enhanced_content
                used_meta["instruction_type"] = "system"
                used_meta["system_instructions"] = used_instruction
                # ğŸ†• æ–°å­—æ®µå†™å…¥é€»è¾‘ï¼šè®°å½•æœ¬è½®å®é™…ä½¿ç”¨çš„æŒ‡ä»¤ï¼ˆä¾›ä¸Šå±‚å­˜å…¥ messages.instructionsï¼‰
                used_meta["instructions"] = used_instruction
                if apply_enhancement:
                    print(f"âœ… å·²ä¸ºç¬¬{user_turn_count}è½®å¯¹è¯æ·»åŠ ç³»ç»Ÿå¢å¼ºæŒ‡ä»¤ï¼ˆæµå¼ï¼‰")
        elif user_turn_count >= 4 and messages:
            # ç¬¬4è½®åŠä»¥åï¼šä½¿ç”¨æŒç»­æŒ‡ä»¤
            last_user_msg_index = self._find_last_user_message_index(messages)
            if last_user_msg_index is not None:
                original_content = messages[last_user_msg_index]["content"]
                enhanced_content, used_instruction = self._enhance_user_message_with_instruction(
                    original_content, 
                    original_content,
                    instruction_type="ongoing"
                )
                if apply_enhancement:
                    messages[last_user_msg_index]["content"] = enhanced_content
                used_meta["instruction_type"] = "ongoing"
                used_meta["ongoing_instructions"] = used_instruction
                # ğŸ†• æ–°å­—æ®µå†™å…¥é€»è¾‘ï¼šè®°å½•æœ¬è½®å®é™…ä½¿ç”¨çš„æŒ‡ä»¤ï¼ˆä¾›ä¸Šå±‚å­˜å…¥ messages.instructionsï¼‰
                used_meta["instructions"] = used_instruction
                if apply_enhancement:
                    print(f"âœ… å·²ä¸ºç¬¬{user_turn_count}è½®å¯¹è¯æ·»åŠ æŒç»­å¢å¼ºæŒ‡ä»¤ï¼ˆæµå¼ï¼‰")
        
        print(f"ğŸ”§ æ„å»ºå®Œæ•´æ¶ˆæ¯åˆ—è¡¨ | æ€»æ¶ˆæ¯æ•°: {len(messages)}")
        print("ğŸ§ " + "="*48)

        # æ¨¡æ‹Ÿè¶…æ—¶
        if random.random() < 0.01:
            raise TimeoutError("4004: ç”Ÿæˆè¶…æ—¶")

        # å¼€å§‹è®¡æ—¶
        start = time.time()
        
        # æµå¼ç”Ÿæˆå¹¶é€æ­¥è¿”å›
        chunk_count = 0
        total_chars = 0
        # é€‰æ‹©è°ƒç”¨å™¨ä¸æ¨¡å‹
        use_caller = caller or self._select_default_caller()
        use_model = model_name
        if use_caller is None:
            raise RuntimeError("æœªé…ç½®ä»»ä½•å¯ç”¨çš„AIè°ƒç”¨å™¨ï¼ˆGrok/Novelï¼‰")

        # ğŸ†• æ–°å­—æ®µå†™å…¥é€»è¾‘ï¼šè¡¥å……å›è°ƒå…ƒæ•°æ®ï¼ˆæ¨¡å‹åä¸æœ¬æ¬¡è°ƒç”¨çš„ä¸Šä¸‹æ–‡è½½è·ï¼‰
        try:
            used_meta["model_name"] = model_name
            # 100% å¤ç°ï¼šè®°å½•æœ¬æ¬¡å®é™…æŠ•å–‚çš„å®Œæ•´ messages
            used_meta["final_messages"] = list(messages)
            used_meta["prompt_payload"] = {
                "system_prompt": role_data.get("system_prompt") if isinstance(role_data, dict) else None,
                "history": history,
                "user_input": user_input,
                "instructions": used_meta.get("instructions"),
                "instruction_type": used_meta.get("instruction_type"),
                # å…¼å®¹æ—§å­—æ®µçš„åŒæ—¶ï¼ŒåŠ å…¥æœ€ç»ˆ messages
                "final_messages": list(messages)
            }
        except Exception:
            pass

        # åœ¨å¼€å§‹æµå¼ä¹‹å‰ï¼Œå›è°ƒä¸€æ¬¡æä¾›æŒ‡ä»¤ä½¿ç”¨çš„å…ƒæ•°æ®
        if on_used_instructions and used_meta.get("instruction_type") is not None:
            try:
                on_used_instructions(dict(used_meta))
            except Exception as _e:
                print(f"âš ï¸ on_used_instructions å›è°ƒæ‰§è¡Œå¤±è´¥: {_e}")

        async for partial_reply in use_caller.get_stream_response(messages, use_model, timeout=timeout):
            chunk_count += 1
            total_chars += len(partial_reply)
            safe_chunk_preview = self._safe_for_logging(partial_reply, 50)
            # print(f"ğŸ”„ æ”¶åˆ°chunk #{chunk_count}: {len(partial_reply)} å­—ç¬¦ | å†…å®¹é¢„è§ˆ: {safe_chunk_preview}...")
            yield partial_reply

        # ç»“æŸæµå¼ç”Ÿæˆ
        print(f"ğŸ¤– AIæµå¼ç”Ÿæˆå®Œæˆ | è€—æ—¶: {time.time() - start:.2f}ç§’ | æ€»chunkæ•°: {chunk_count} | æ€»å­—ç¬¦æ•°: {total_chars}")
        print("ğŸ¤–" + "="*48)

    async def generate_reply_stream_with_retry(self, role_data, history, user_input, 
                                             max_retries=3, timeout=60, session_context_source=None,
                                             on_used_instructions: Optional[Callable[[Dict[str, Any]], None]] = None,
                                             apply_enhancement: bool = False) -> AsyncGenerator[str, None]:
        """
        å¸¦é‡è¯•æœºåˆ¶çš„æµå¼ç”ŸæˆAIå›å¤
        
        Args:
            role_data: è§’è‰²é…ç½®æ•°æ®
            history: ä¼šè¯å†å²æ¶ˆæ¯
            user_input: å½“å‰ç”¨æˆ·è¾“å…¥
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤3æ¬¡
            timeout: è¶…æ—¶æ—¶é—´
            session_context_source: ä¼šè¯ä¸Šä¸‹æ–‡æ¥æºæ ‡è®°
            on_used_instructions: å¯é€‰å›è°ƒï¼Œæºå¸¦æœ¬æ¬¡è°ƒç”¨å®é™…ä½¿ç”¨çš„æŒ‡ä»¤å…ƒæ•°æ®ï¼ˆä»…åœ¨æˆåŠŸçš„é‚£æ¬¡å°è¯•è§¦å‘ä¸€æ¬¡ï¼‰
            apply_enhancement: æ˜¯å¦åœ¨æœ¬æ–¹æ³•ä¸­å¯¹æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯åšæŒ‡ä»¤å¢å¼ºï¼ˆé»˜è®¤ Falseï¼‰
            
        Yields:
            str: æ¯ä¸ªæµå¼å›å¤ç‰‡æ®µ
        """
        for attempt in range(max_retries):
            try:
                print(f"ğŸ”„ AIç”Ÿæˆå°è¯• #{attempt + 1}/{max_retries}")

                # å‰ä¸¤æ¬¡ä½¿ç”¨ Grokï¼Œç¬¬ä¸‰æ¬¡ä½¿ç”¨ Novel
                if attempt < 2:
                    if not self.grok:
                        raise RuntimeError("Grok è°ƒç”¨å™¨æœªé…ç½®")
                    provider = "Grok"
                    caller = self.grok
                    model_env = os.getenv("GROK_MODEL")
                else:
                    if not self.novel:
                        raise RuntimeError("Novel è°ƒç”¨å™¨æœªé…ç½®")
                    provider = "Novel"
                    caller = self.novel
                    model_env = os.getenv("NOVEL_MODEL")

                print(f"ğŸš€ æœ¬æ¬¡å°è¯•ä½¿ç”¨æä¾›æ–¹: {provider} | æ¨¡å‹: {model_env}")

                # ä»…åœ¨æˆåŠŸå¼€å§‹äº§å‡ºå†…å®¹åå†å¯¹ä¸Šå±‚è§¦å‘å›è°ƒï¼Œé¿å…å¤±è´¥å°è¯•æ±¡æŸ“
                used_meta_candidate: Dict[str, Any] = {}
                def _capture_used_instructions(meta: Dict[str, Any]) -> None:
                    # è®°å½•å€™é€‰å…ƒæ•°æ®ï¼Œç¨ååœ¨é¦–æ¬¡äº§å‡ºæ—¶ç»Ÿä¸€ä¸ŠæŠ¥
                    used_meta_candidate.clear()
                    used_meta_candidate.update(meta or {})
                    # å¢è¡¥ provider/model
                    used_meta_candidate["provider"] = provider
                    used_meta_candidate["model"] = model_env

                # ä½¿ç”¨ç»Ÿä¸€çš„è¶…æ—¶ç­–ç•¥ï¼ˆä¸¤è¾¹ caller éƒ½ä½¿ç”¨ total=timeoutï¼‰
                first_chunk_sent = False
                async for chunk in self.generate_reply_stream(
                    role_data=role_data,
                    history=history,
                    user_input=user_input,
                    timeout=timeout,
                    session_context_source=session_context_source,
                    caller=caller,
                    model_name=model_env,
                    on_used_instructions=_capture_used_instructions,
                    apply_enhancement=apply_enhancement
                ):
                    if not first_chunk_sent:
                        # é¦–æ¬¡äº§å‡ºå†…å®¹æ—¶å†æŠŠæœ¬æ¬¡å°è¯•çš„å…ƒæ•°æ®ä¸ŠæŠ¥ç»™è°ƒç”¨æ–¹
                        if on_used_instructions and used_meta_candidate:
                            try:
                                on_used_instructions(dict(used_meta_candidate))
                            except Exception as _e:
                                print(f"âš ï¸ on_used_instructions å›è°ƒæ‰§è¡Œå¤±è´¥: {_e}")
                        first_chunk_sent = True
                    yield chunk

                # æˆåŠŸç”Ÿæˆï¼Œé€€å‡ºé‡è¯•å¾ªç¯
                print(f"âœ… AIç”ŸæˆæˆåŠŸï¼ˆç¬¬{attempt + 1}æ¬¡å°è¯•ï¼Œæä¾›æ–¹: {provider}ï¼‰")
                return

            except Exception as e:
                print(f"âŒ AIç”Ÿæˆå¤±è´¥ï¼ˆç¬¬{attempt + 1}æ¬¡å°è¯•ï¼‰: {e}")

                if attempt == max_retries - 1:
                    # æœ€åä¸€æ¬¡é‡è¯•å¤±è´¥ï¼Œè¿”å›å›ºå®šè¯æœ¯
                    print(f"ğŸ’” æ‰€æœ‰é‡è¯•å‡å¤±è´¥ï¼Œè¿”å›å…œåº•è¯æœ¯")
                    yield "æŠ±æ­‰ï¼Œå›å¤å‡ºç°äº†é—®é¢˜ï¼Œåå°æ­£åœ¨åŠ ç´§ä¿®å¤ï¼Œè¯·è€å¿ƒç­‰å¾…"
                    return
                else:
                    # ç»§ç»­é‡è¯•
                    print(f"ğŸ”„ å‡†å¤‡è¿›è¡Œç¬¬{attempt + 2}æ¬¡é‡è¯•...")
                    continue

    def _safe_for_logging(self, text: str, max_length: int = 50) -> str:
        """å®‰å…¨åœ°æˆªæ–­æ–‡æœ¬ç”¨äºæ—¥å¿—è¾“å‡º"""
        if not text:
            return ""
        if len(text) <= max_length:
            return text
        return text[:max_length] + "..."

    def _select_default_caller(self) -> Optional[object]:
        """
        é€‰æ‹©ä¸€ä¸ªé»˜è®¤å¯ç”¨çš„è°ƒç”¨å™¨ï¼š
        ä¼˜å…ˆ Novelï¼Œå…¶æ¬¡ Grokï¼›å¦‚æœéƒ½ä¸å­˜åœ¨åˆ™è¿”å› None
        """
        if self.novel:
            return self.novel
        if self.grok:
            return self.grok
        return None
    
    # get_last_used_instructions å·²åºŸå¼ƒï¼ˆç§»é™¤ï¼‰


# âœ… å…¨å±€å”¯ä¸€å®ä¾‹ï¼ˆä¸´æ—¶å ä½ï¼Œå®é™…ä½¿ç”¨æ—¶åº”é€šè¿‡å®¹å™¨è·å–ï¼‰
# åœ¨åº”ç”¨å¯åŠ¨æ—¶ï¼Œåº”è¯¥é€šè¿‡å®¹å™¨åˆ›å»ºå¹¶æ›¿æ¢è¿™ä¸ªå®ä¾‹
ai_completion_port = None  # å°†åœ¨å®¹å™¨ä¸­åˆå§‹åŒ–
