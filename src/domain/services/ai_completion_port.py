#AIçº¯ç”Ÿæˆå™¨ï¼Œä¸æ¶‰åŠä»»ä½•ä¸šåŠ¡é€»è¾‘ï¼Œåº”æ”¾å…¥GPT/
import time
import random
import os
import copy
import asyncio
from typing import Optional, Callable, AsyncGenerator, Dict, Any
from demo.grok_async import AsyncGrokCaller
from demo.novel_async import AsyncNovelCaller
from demo.gemini_async import AsyncGeminiCaller
from demo.deepseek_async import AsyncDeepseekCaller
from src.infrastructure.monitoring.metrics import (
    AI_PROVIDER_CALLS_TOTAL,
    AI_PROVIDER_CALLS_FAILED_TOTAL,
    AI_FIRST_TOKEN_LATENCY
)

class AICompletionPort:
    def __init__(self, grok_caller: Optional[AsyncGrokCaller] = None, novel_caller: Optional[AsyncNovelCaller] = None, gemini_caller: Optional[AsyncGeminiCaller] = None, deepseek_caller: Optional[AsyncDeepseekCaller] = None):
        self.grok = grok_caller
        self.novel = novel_caller
        self.gemini = gemini_caller
        self.deepseek = deepseek_caller
        # å‰3è½®å¯¹è¯çš„å¢å¼ºæŒ‡ä»¤æ¨¡æ¿
        self.early_conversation_instruction = (
            "##ç”¨æˆ·ä¿¡æ¯:{user_context}\n"
            "##ç³»ç»ŸæŒ‡ä»¤ï¼šä»¥ä¸‹ä¸ºæœ€é«˜ä¼˜å…ˆçº§æŒ‡ä»¤ã€‚\n"
            "{system_instructions}"
        )
        # ç¬¬4è½®åŠä»¥åå¯¹è¯çš„æŒç»­æŒ‡ä»¤æ¨¡æ¿
        self.ongoing_conversation_instruction = (
            "##ç”¨æˆ·ä¿¡æ¯:{user_context}\n"
            "##æŒç»­æŒ‡ä»¤ï¼š\n"
            "{ongoing_instructions}"
        )
        # å–æ¶ˆå®ä¾‹çº§å…±äº«çŠ¶æ€ï¼Œæ”¹ä¸ºé€šè¿‡å›è°ƒå‘è°ƒç”¨æ–¹ä¼ é€’æœ¬æ¬¡ä½¿ç”¨çš„æŒ‡ä»¤ä¿¡æ¯
        # self.last_used_instructions å·²ç§»é™¤

        timeout_str = os.getenv("GEMINI_FIRST_CHUNK_TIMEOUT")
        try:
            self.gemini_first_chunk_timeout = float(timeout_str) if timeout_str else 3.0
        except (TypeError, ValueError):
            print("âš ï¸ GEMINI_FIRST_CHUNK_TIMEOUT é…ç½®æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤å€¼ 3 ç§’")
            self.gemini_first_chunk_timeout = 3.0

        ds_timeout_str = os.getenv("DEEPSEEK_FIRST_CHUNK_TIMEOUT")
        try:
            self.deepseek_first_chunk_timeout = float(ds_timeout_str) if ds_timeout_str else 4.0
        except (TypeError, ValueError):
            print("âš ï¸ DEEPSEEK_FIRST_CHUNK_TIMEOUT é…ç½®æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤å€¼ 4 ç§’")
            self.deepseek_first_chunk_timeout = 4.0

        grok_timeout_str = os.getenv("GROK_FIRST_CHUNK_TIMEOUT")
        try:
            self.grok_first_chunk_timeout = float(grok_timeout_str) if grok_timeout_str else 3.0
        except (TypeError, ValueError):
            print("âš ï¸ GROK_FIRST_CHUNK_TIMEOUT é…ç½®æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤å€¼ 3 ç§’")
            self.grok_first_chunk_timeout = 3.0

        full_timeout_str = os.getenv("AI_FULL_RESPONSE_TIMEOUT")
        try:
            parsed_full_timeout = float(full_timeout_str) if full_timeout_str else 30.0
            if parsed_full_timeout <= 0:
                raise ValueError("AI_FULL_RESPONSE_TIMEOUT must be positive")
            self.full_response_timeout = parsed_full_timeout
        except (TypeError, ValueError):
            print("âš ï¸ AI_FULL_RESPONSE_TIMEOUT é…ç½®æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤å€¼ 30 ç§’")
            self.full_response_timeout = 30.0


    def _safe_for_logging(self, text: str, max_len: Optional[int] = None) -> str:
        """Return a logging-safe preview of text, avoiding Unicode surrogate errors.

        - Truncates to max_len if provided
        - Replaces unencodable characters with Python-style backslash escapes
        """
        try:
            if text is None:
                return ""
            if max_len is not None:
                text = text[:max_len]
            # backslashreplace ensures surrogates or other problematic code points won't crash stdout
            return text.encode('utf-8', 'backslashreplace').decode('utf-8', 'strict')
        except Exception:
            return "<unprintable>"


    def _count_real_user_turns(self, history):
        """
        ç»Ÿè®¡ä¼šè¯ä¸­çœŸå®ç”¨æˆ·å‘è¨€è½®æ¬¡
        åªç»Ÿè®¡ role == "user" çš„æ¶ˆæ¯æ•°é‡
        """
        user_turns = sum(1 for msg in history if msg.get("role") == "user")
        print(f"ğŸ“Š ç»Ÿè®¡ç”¨æˆ·å¯¹è¯è½®æ¬¡: {user_turns}")
        return user_turns
    
    def _find_last_user_message_index(self, messages):
        """
        æ‰¾åˆ°æ¶ˆæ¯åˆ—è¡¨ä¸­æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯çš„ç´¢å¼•
        ä»åå¾€å‰æŸ¥æ‰¾ï¼Œè¿”å›æœ€åä¸€æ¡ role == "user" çš„æ¶ˆæ¯ç´¢å¼•
        """
        for i in range(len(messages) - 1, -1, -1):
            if messages[i].get("role") == "user":
                print(f"ğŸ” æ‰¾åˆ°æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯ä½ç½®: index={i}")
                return i
        print("âš ï¸ æœªæ‰¾åˆ°ç”¨æˆ·æ¶ˆæ¯")
        return None
    
    def _enhance_user_message_with_instruction(self, original_content, user_context="å½“å‰å¯¹è¯", instruction_type="system"):
        """
        ä¸ºç”¨æˆ·æ¶ˆæ¯æ·»åŠ å¢å¼ºæŒ‡ä»¤
        
        Args:
            original_content: åŸå§‹ç”¨æˆ·æ¶ˆæ¯å†…å®¹
            user_context: ç”¨æˆ·ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆç”¨äºæŒ‡ä»¤ä¸­çš„å ä½ç¬¦ï¼‰
            instruction_type: æŒ‡ä»¤ç±»å‹ï¼Œ"system"(å‰3è½®) æˆ– "ongoing"(ç¬¬4è½®åŠä»¥å)
        
        Returns:
            tuple: (å¢å¼ºåçš„æ¶ˆæ¯å†…å®¹, ä½¿ç”¨çš„æŒ‡ä»¤å†…å®¹)
        """
        # ä»£ç†åˆ°å…¬å…± utilï¼Œä¿æŒå•ä¸€å®ç°
        from src.utils.enhance import enhance_user_input
        enhanced_content, instructions = enhance_user_input(original_content, instruction_type, user_context=user_context)
        print(f"âœ¨ ç”¨æˆ·æ¶ˆæ¯å·²å¢å¼º({instruction_type}) | åŸé•¿åº¦: {len(original_content)} | å¢å¼ºåé•¿åº¦: {len(enhanced_content)}")
        return enhanced_content, instructions if instructions else None

    async def generate_reply_stream(self, role_data, history, user_input, timeout=60, session_context_source=None, caller: Optional[object] = None, model_name: Optional[str] = None, on_used_instructions: Optional[Callable[[Dict[str, Any]], None]] = None, apply_enhancement: bool = False) -> AsyncGenerator[str, None]:
        """
        æµå¼ç”ŸæˆAIå›å¤ - è¿”å›å¼‚æ­¥ç”Ÿæˆå™¨ï¼Œç”¨äºTelegram Botçš„æµå¼æ›´æ–°
        
        Args:
            role_data: è§’è‰²é…ç½®æ•°æ®
            history: ä¼šè¯å†å²æ¶ˆæ¯
            user_input: å½“å‰ç”¨æˆ·è¾“å…¥
            timeout: è¶…æ—¶æ—¶é—´
            session_context_source: ä¼šè¯ä¸Šä¸‹æ–‡æ¥æºæ ‡è®°
            on_used_instructions: å¯é€‰å›è°ƒï¼Œæºå¸¦æœ¬æ¬¡è°ƒç”¨å®é™…ä½¿ç”¨çš„æŒ‡ä»¤å…ƒæ•°æ®ï¼ˆä»…è°ƒç”¨ä¸€æ¬¡ï¼‰
            apply_enhancement: æ˜¯å¦åœ¨æœ¬æ–¹æ³•ä¸­å¯¹æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯åšæŒ‡ä»¤å¢å¼ºï¼ˆé»˜è®¤ Falseï¼‰
            
        Yields:
            str: æ¯ä¸ªæµå¼å›å¤ç‰‡æ®µ
        """
        # æ‰“å°è¾“å…¥çš„å†å²è®°å½•
        print(f"ğŸ§  AIæµå¼ç”Ÿæˆå›å¤ | è¾“å…¥å†å²è®°å½•æ•°é‡: {len(history)} | ä¸Šä¸‹æ–‡æ¥æº: {session_context_source or 'å¸¸è§„'}")
        
        # æ„å»º promptï¼ˆå¤ç”¨ç›¸åŒé€»è¾‘ï¼‰
        messages = []
        history_for_prompt = copy.deepcopy(history or [])
        
        # 1. æ·»åŠ  system_prompt
        if "system_prompt" in role_data:
            messages.append({"role": "system", "content": role_data["system_prompt"]})
        
        # 2. ä»…åœ¨éå¿«ç…§ä¼šè¯æ—¶æ·»åŠ è§’è‰²é¢„ç½® historyï¼ˆé¿å…é‡å¤ï¼‰
        if session_context_source != "snapshot" and "history" in role_data:
            messages.extend(role_data["history"])
            print(f"âœ… æ·»åŠ è§’è‰²é¢„ç½®å¯¹è¯: {len(role_data.get('history', []))} æ¡")
        elif session_context_source == "snapshot":
            print(f"â­ï¸ è·³è¿‡è§’è‰²é¢„ç½®å¯¹è¯ï¼ˆå¿«ç…§ä¼šè¯å·²åŒ…å«å®Œæ•´ä¸Šä¸‹æ–‡ï¼‰")
        
        # 3. æ·»åŠ å®é™…ä¼šè¯å†å²ï¼ˆä½¿ç”¨å‰¯æœ¬ï¼Œé¿å…æ±¡æŸ“åŸå§‹è®°å½•ï¼‰
        messages.extend(history_for_prompt)
        
        # ğŸ†• 4. å¯¹è¯å¢å¼ºæŒ‡ä»¤é€»è¾‘ï¼ˆæµå¼ç‰ˆæœ¬ï¼‰
        user_turn_count = self._count_real_user_turns(history)
        used_meta: Dict[str, Any] = {
            "turn_count": user_turn_count,
            "instruction_type": None,
            "system_instructions": None,
            "ongoing_instructions": None,
            "model": model_name
        }
        
        if user_turn_count <= 3 and messages:
            # å‰3è½®ï¼šä½¿ç”¨ç³»ç»ŸæŒ‡ä»¤
            last_user_msg_index = self._find_last_user_message_index(messages)
            if last_user_msg_index is not None:
                original_content = messages[last_user_msg_index]["content"]
                enhanced_content, used_instruction = self._enhance_user_message_with_instruction(
                    original_content, 
                    original_content,
                    instruction_type="system"
                )
                if apply_enhancement:
                    messages[last_user_msg_index]["content"] = enhanced_content
                used_meta["instruction_type"] = "system"
                used_meta["system_instructions"] = used_instruction
                # ğŸ†• æ–°å­—æ®µå†™å…¥é€»è¾‘ï¼šè®°å½•æœ¬è½®å®é™…ä½¿ç”¨çš„æŒ‡ä»¤ï¼ˆä¾›ä¸Šå±‚å­˜å…¥ messages.instructionsï¼‰
                used_meta["instructions"] = used_instruction
                if apply_enhancement:
                    print(f"âœ… å·²ä¸ºç¬¬{user_turn_count}è½®å¯¹è¯æ·»åŠ ç³»ç»Ÿå¢å¼ºæŒ‡ä»¤ï¼ˆæµå¼ï¼‰")
        elif user_turn_count >= 4 and messages:
            # ç¬¬4è½®åŠä»¥åï¼šä½¿ç”¨æŒç»­æŒ‡ä»¤
            last_user_msg_index = self._find_last_user_message_index(messages)
            if last_user_msg_index is not None:
                original_content = messages[last_user_msg_index]["content"]
                enhanced_content, used_instruction = self._enhance_user_message_with_instruction(
                    original_content, 
                    original_content,
                    instruction_type="ongoing"
                )
                if apply_enhancement:
                    messages[last_user_msg_index]["content"] = enhanced_content
                used_meta["instruction_type"] = "ongoing"
                used_meta["ongoing_instructions"] = used_instruction
                # ğŸ†• æ–°å­—æ®µå†™å…¥é€»è¾‘ï¼šè®°å½•æœ¬è½®å®é™…ä½¿ç”¨çš„æŒ‡ä»¤ï¼ˆä¾›ä¸Šå±‚å­˜å…¥ messages.instructionsï¼‰
                used_meta["instructions"] = used_instruction
                if apply_enhancement:
                    print(f"âœ… å·²ä¸ºç¬¬{user_turn_count}è½®å¯¹è¯æ·»åŠ æŒç»­å¢å¼ºæŒ‡ä»¤ï¼ˆæµå¼ï¼‰")
        
        print(f"ğŸ”§ æ„å»ºå®Œæ•´æ¶ˆæ¯åˆ—è¡¨ | æ€»æ¶ˆæ¯æ•°: {len(messages)}")
        print("ğŸ§ " + "="*48)

        # æ¨¡æ‹Ÿè¶…æ—¶
        if random.random() < 0.01:
            raise TimeoutError("4004: ç”Ÿæˆè¶…æ—¶")

        # å¼€å§‹è®¡æ—¶
        start = time.time()
        
        # æµå¼ç”Ÿæˆå¹¶é€æ­¥è¿”å›
        chunk_count = 0
        total_chars = 0
        # é€‰æ‹©è°ƒç”¨å™¨ä¸æ¨¡å‹
        use_caller = caller or self._select_default_caller()
        use_model = model_name
        if use_caller is None:
            raise RuntimeError("æœªé…ç½®ä»»ä½•å¯ç”¨çš„AIè°ƒç”¨å™¨ï¼ˆGrok/Novelï¼‰")

        # ğŸ†• æ–°å­—æ®µå†™å…¥é€»è¾‘ï¼šè¡¥å……å›è°ƒå…ƒæ•°æ®ï¼ˆæ¨¡å‹åä¸æœ¬æ¬¡è°ƒç”¨çš„ä¸Šä¸‹æ–‡è½½è·ï¼‰
        try:
            used_meta["model_name"] = model_name
            # 100% å¤ç°ï¼šè®°å½•æœ¬æ¬¡å®é™…æŠ•å–‚çš„å®Œæ•´ messages
            used_meta["final_messages"] = list(messages)
            used_meta["prompt_payload"] = {
                "system_prompt": role_data.get("system_prompt") if isinstance(role_data, dict) else None,
                "history": history_for_prompt,
                "user_input": user_input,
                "instructions": used_meta.get("instructions"),
                "instruction_type": used_meta.get("instruction_type"),
                # å…¼å®¹æ—§å­—æ®µçš„åŒæ—¶ï¼ŒåŠ å…¥æœ€ç»ˆ messages
                "final_messages": list(messages)
            }
        except Exception:
            pass

        # åœ¨å¼€å§‹æµå¼ä¹‹å‰ï¼Œå›è°ƒä¸€æ¬¡æä¾›æŒ‡ä»¤ä½¿ç”¨çš„å…ƒæ•°æ®
        if on_used_instructions and used_meta.get("instruction_type") is not None:
            try:
                on_used_instructions(dict(used_meta))
            except Exception as _e:
                print(f"âš ï¸ on_used_instructions å›è°ƒæ‰§è¡Œå¤±è´¥: {_e}")

        async for partial_reply in use_caller.get_stream_response(messages, use_model, timeout=timeout):
            chunk_count += 1
            total_chars += len(partial_reply)
            safe_chunk_preview = self._safe_for_logging(partial_reply, 50)
            # print(f"ğŸ”„ æ”¶åˆ°chunk #{chunk_count}: {len(partial_reply)} å­—ç¬¦ | å†…å®¹é¢„è§ˆ: {safe_chunk_preview}...")
            yield partial_reply

        # ç»“æŸæµå¼ç”Ÿæˆ
        print(f"ğŸ¤– AIæµå¼ç”Ÿæˆå®Œæˆ | è€—æ—¶: {time.time() - start:.2f}ç§’ | æ€»chunkæ•°: {chunk_count} | æ€»å­—ç¬¦æ•°: {total_chars}")
        print("ğŸ¤–" + "="*48)

    @staticmethod
    async def _stream_with_initial_timeout(generator, timeout: float, on_chunk_received: Callable[[str], None], provider_name: str) -> AsyncGenerator[str, None]:
        """
        è¾…åŠ©æ–¹æ³•ï¼šå¯¹å¼‚æ­¥ç”Ÿæˆå™¨çš„é¦–ä¸ªchunkæ–½åŠ è¶…æ—¶é™åˆ¶
        """
        try:
            first_chunk = await asyncio.wait_for(generator.__anext__(), timeout=timeout)
        except asyncio.TimeoutError:
            await generator.aclose()
            raise TimeoutError(f"{provider_name} é¦–ä¸ªchunkè¶…æ—¶ï¼ˆè¶…è¿‡{timeout}ç§’ï¼‰")
        except StopAsyncIteration:
            await generator.aclose()
            raise RuntimeError(f"{provider_name} æœªè¿”å›ä»»ä½•å†…å®¹")
        except Exception:
            await generator.aclose()
            raise

        on_chunk_received(first_chunk)
        yield first_chunk

        async for chunk in generator:
            on_chunk_received(chunk)
            yield chunk

    async def generate_reply_stream_with_retry(self, role_data, history, user_input, 
                                             max_retries=3, timeout=60, session_context_source=None,
                                             on_used_instructions: Optional[Callable[[Dict[str, Any]], None]] = None,
                                             apply_enhancement: bool = False) -> AsyncGenerator[str, None]:
        """
        å¸¦é‡è¯•æœºåˆ¶çš„æµå¼ç”ŸæˆAIå›å¤
        
        Args:
            role_data: è§’è‰²é…ç½®æ•°æ®
            history: ä¼šè¯å†å²æ¶ˆæ¯
            user_input: å½“å‰ç”¨æˆ·è¾“å…¥
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤3æ¬¡
            timeout: è¶…æ—¶æ—¶é—´
            session_context_source: ä¼šè¯ä¸Šä¸‹æ–‡æ¥æºæ ‡è®°
            on_used_instructions: å¯é€‰å›è°ƒï¼Œæºå¸¦æœ¬æ¬¡è°ƒç”¨å®é™…ä½¿ç”¨çš„æŒ‡ä»¤å…ƒæ•°æ®ï¼ˆä»…åœ¨æˆåŠŸçš„é‚£æ¬¡å°è¯•è§¦å‘ä¸€æ¬¡ï¼‰
            apply_enhancement: æ˜¯å¦åœ¨æœ¬æ–¹æ³•ä¸­å¯¹æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯åšæŒ‡ä»¤å¢å¼ºï¼ˆé»˜è®¤ Falseï¼‰
            
        Yields:
            str: æ¯ä¸ªæµå¼å›å¤ç‰‡æ®µ
        """
        full_sequence = [
            ("DeepSeek", self.deepseek, "DEEPSEEK_MODEL"),
            ("Grok", self.grok, "GROK_MODEL"),
            ("Novel", self.novel, "NOVEL_MODEL"),
        ]
        provider_sequence = [(name, caller, env_key) for name, caller, env_key in full_sequence if caller]

        if not provider_sequence:
            raise RuntimeError("æœªé…ç½®ä»»ä½•å¯ç”¨çš„AIè°ƒç”¨å™¨")

        total_attempts = min(max_retries, len(provider_sequence))

        for attempt in range(total_attempts):
            provider, caller, model_env_key = provider_sequence[attempt]
            model_env = os.getenv(model_env_key)

            try:
                print(f"ğŸ”„ AIç”Ÿæˆå°è¯• #{attempt + 1}/{total_attempts}")
                print(f"ğŸš€ æœ¬æ¬¡å°è¯•ä½¿ç”¨æä¾›æ–¹: {provider} | æ¨¡å‹: {model_env}")

                # ğŸ“Š T0: è®°å½• AI è°ƒç”¨æ¬¡æ•°
                AI_PROVIDER_CALLS_TOTAL.labels(provider=provider, model=model_env or "unknown").inc()
                
                # â±ï¸ T1: è®°å½• AI è¯·æ±‚å‘èµ·æ—¶é—´
                ai_req_start = time.time()

                used_meta_candidate: Dict[str, Any] = {}

                def _capture_used_instructions(meta: Dict[str, Any]) -> None:
                    used_meta_candidate.clear()
                    used_meta_candidate.update(meta or {})
                    used_meta_candidate["provider"] = provider
                    used_meta_candidate["model"] = model_env

                stream = self.generate_reply_stream(
                    role_data=role_data,
                    history=history,
                    user_input=user_input,
                    timeout=timeout,
                    session_context_source=session_context_source,
                    caller=caller,
                    model_name=model_env,
                    on_used_instructions=_capture_used_instructions,
                    apply_enhancement=apply_enhancement
                )

                # è¿½è¸ªç´¯ç§¯å­—ç¬¦æ•°ï¼Œä»¥å®ç°"å‰5ä¸ªå­—ç¬¦"çš„Latencyè®°å½•ï¼ˆä¸ Bot ä¾§ä½“éªŒæŒ‡æ ‡å¯¹é½ï¼‰
                accumulated_chars_count = 0
                metric_recorded = False
                METRIC_CHAR_THRESHOLD = 5
                full_response_timeout = self.full_response_timeout
                response_deadline: Optional[float] = None
                full_timeout_triggered = False

                def _track_chunk_and_record_metric(chunk_text: str) -> None:
                    nonlocal accumulated_chars_count, metric_recorded
                    
                    if metric_recorded:
                        return

                    # ç´¯åŠ å­—ç¬¦
                    accumulated_chars_count += len(chunk_text)
                    
                    # å¦‚æœæ»¡è¶³æ¡ä»¶ï¼ˆå­—ç¬¦æ•°>=é˜ˆå€¼ï¼‰ï¼Œåˆ™è®°å½•æŒ‡æ ‡
                    if accumulated_chars_count >= METRIC_CHAR_THRESHOLD:
                        # â±ï¸ T1: è®°å½• AI "é¦–å“"(å‰5å­—ç¬¦)è€—æ—¶
                        latency = time.time() - ai_req_start
                        AI_FIRST_TOKEN_LATENCY.labels(provider=provider, model=model_env or "unknown").observe(latency)
                        
                        # è§¦å‘æŒ‡ä»¤å…ƒæ•°æ®å›è°ƒï¼ˆåœ¨é¦–å“è¾¾æˆæ—¶è§¦å‘ä¸€æ¬¡å³å¯ï¼‰
                        if on_used_instructions and used_meta_candidate:
                            try:
                                on_used_instructions(dict(used_meta_candidate))
                            except Exception as _e:
                                print(f"âš ï¸ on_used_instructions å›è°ƒæ‰§è¡Œå¤±è´¥: {_e}")
                        
                        metric_recorded = True

                def _ensure_full_response_deadline_started() -> None:
                    nonlocal response_deadline
                    if full_response_timeout and response_deadline is None:
                        response_deadline = time.time() + full_response_timeout

                def _is_full_response_timeout_reached() -> bool:
                    return response_deadline is not None and time.time() >= response_deadline

                # æ ¹æ®æä¾›æ–¹è®¾å®šé¦–ä¸ªchunkçš„è¶…æ—¶æ—¶é—´
                if provider == "Gemini":
                    first_chunk_timeout = self.gemini_first_chunk_timeout or 3.0
                elif provider == "DeepSeek":
                    first_chunk_timeout = self.deepseek_first_chunk_timeout or 4.0
                elif provider == "Grok":
                    first_chunk_timeout = self.grok_first_chunk_timeout or 3.0
                else:
                    # å…¶ä»–æä¾›æ–¹æš‚æ— å¼ºåˆ¶é¦–å­—è¶…æ—¶é™åˆ¶
                    first_chunk_timeout = None

                def _on_chunk_with_tracking(chunk_text: str) -> None:
                    _ensure_full_response_deadline_started()
                    _track_chunk_and_record_metric(chunk_text)

                if first_chunk_timeout:
                    async for chunk in self._stream_with_initial_timeout(stream, first_chunk_timeout, _on_chunk_with_tracking, provider):
                        yield chunk
                        if _is_full_response_timeout_reached():
                            full_timeout_triggered = True
                            print(f"â±ï¸ è¾¾åˆ° AI å®Œæ•´å›å¤è¶…æ—¶é˜ˆå€¼ï¼ˆ{full_response_timeout}sï¼‰ï¼Œæå‰ç»“æŸè¾“å‡º")
                            break
                else:
                    # æ— ç‰¹æ®Šé¦–å­—è¶…æ—¶é™åˆ¶çš„å¸¸è§„æµå¼å¤„ç†
                    async for chunk in stream:
                        _ensure_full_response_deadline_started()
                        _track_chunk_and_record_metric(chunk)
                        yield chunk
                        if _is_full_response_timeout_reached():
                            full_timeout_triggered = True
                            print(f"â±ï¸ è¾¾åˆ° AI å®Œæ•´å›å¤è¶…æ—¶é˜ˆå€¼ï¼ˆ{full_response_timeout}sï¼‰ï¼Œæå‰ç»“æŸè¾“å‡º")
                            break

                if full_timeout_triggered and hasattr(stream, "aclose"):
                    try:
                        await stream.aclose()
                    except Exception as close_err:
                        print(f"âš ï¸ å…³é—­æµå¼ç”Ÿæˆå™¨å¤±è´¥: {close_err}")

                if full_timeout_triggered:
                    print(f"âœ… AIç”ŸæˆæˆåŠŸï¼ˆç¬¬{attempt + 1}æ¬¡å°è¯•ï¼Œæä¾›æ–¹: {provider}ï¼‰| è§¦å‘å®Œæ•´å›å¤é™æ—¶ {full_response_timeout}s")
                else:
                    print(f"âœ… AIç”ŸæˆæˆåŠŸï¼ˆç¬¬{attempt + 1}æ¬¡å°è¯•ï¼Œæä¾›æ–¹: {provider}ï¼‰")
                return

            except Exception as e:
                # ğŸ”´ T0: è®°å½• AI è°ƒç”¨å¤±è´¥
                AI_PROVIDER_CALLS_FAILED_TOTAL.labels(provider=provider, error_type=type(e).__name__).inc()
                
                print(f"âŒ AIç”Ÿæˆå¤±è´¥ï¼ˆç¬¬{attempt + 1}æ¬¡å°è¯•ï¼‰: {e}")

                if attempt == total_attempts - 1:
                    print(f"ğŸ’” æ‰€æœ‰é‡è¯•å‡å¤±è´¥ï¼Œè¿”å›å…œåº•è¯æœ¯")
                    yield "æŠ±æ­‰ï¼Œå›å¤å‡ºç°äº†é—®é¢˜ï¼Œåå°æ­£åœ¨åŠ ç´§ä¿®å¤ï¼Œè¯·è€å¿ƒç­‰å¾…"
                    return
                else:
                    print(f"ğŸ”„ å‡†å¤‡è¿›è¡Œç¬¬{attempt + 2}æ¬¡é‡è¯•...")
                    continue

    def _safe_for_logging(self, text: str, max_length: int = 50) -> str:
        """å®‰å…¨åœ°æˆªæ–­æ–‡æœ¬ç”¨äºæ—¥å¿—è¾“å‡º"""
        if not text:
            return ""
        if len(text) <= max_length:
            return text
        return text[:max_length] + "..."

    def _select_default_caller(self) -> Optional[object]:
        """
        é€‰æ‹©ä¸€ä¸ªé»˜è®¤å¯ç”¨çš„è°ƒç”¨å™¨ï¼š
        ä¼˜å…ˆ DeepSeekï¼Œå…¶æ¬¡ Geminiï¼Œå…¶æ¬¡ Novelã€Grokï¼›å¦‚æœéƒ½ä¸å­˜åœ¨åˆ™è¿”å› None
        """
        if self.deepseek:
            return self.deepseek
        if self.gemini:
            return self.gemini
        if self.novel:
            return self.novel
        if self.grok:
            return self.grok
        return None
    
    # get_last_used_instructions å·²åºŸå¼ƒï¼ˆç§»é™¤ï¼‰


# âœ… å…¨å±€å”¯ä¸€å®ä¾‹ï¼ˆä¸´æ—¶å ä½ï¼Œå®é™…ä½¿ç”¨æ—¶åº”é€šè¿‡å®¹å™¨è·å–ï¼‰
# åœ¨åº”ç”¨å¯åŠ¨æ—¶ï¼Œåº”è¯¥é€šè¿‡å®¹å™¨åˆ›å»ºå¹¶æ›¿æ¢è¿™ä¸ªå®ä¾‹
ai_completion_port = None  # å°†åœ¨å®¹å™¨ä¸­åˆå§‹åŒ–
