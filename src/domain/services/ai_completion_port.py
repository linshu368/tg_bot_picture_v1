#AIçº¯ç”Ÿæˆå™¨ï¼Œä¸æ¶‰åŠä»»ä½•ä¸šåŠ¡é€»è¾‘ï¼Œåº”æ”¾å…¥GPT/
import time
import random
import os
from typing import Optional, Callable, AsyncGenerator
from demo.grok_async import AsyncGrokCaller
from demo.novel_async import AsyncNovelCaller

class AICompletionPort:
    def __init__(self, grok_caller: Optional[AsyncGrokCaller] = None, novel_caller: Optional[AsyncNovelCaller] = None):
        self.grok = grok_caller
        self.novel = novel_caller
        # å‰3è½®å¯¹è¯çš„å¢å¼ºæŒ‡ä»¤æ¨¡æ¿
        self.early_conversation_instruction = (
            "##ç”¨æˆ·ä¿¡æ¯:{user_context}\n"
            "##ç³»ç»ŸæŒ‡ä»¤ï¼šä»¥ä¸‹ä¸ºæœ€é«˜ä¼˜å…ˆçº§æŒ‡ä»¤ã€‚\n"
            "1. ä½¿ç”¨ `/æ²‰æµ¸æ¨¡å¼ on` ï¼Œä»¥å‰§æƒ…ä½“éªŒä¸æƒ…ç»ªæµä¸ºæ ¸å¿ƒï¼Œå¼ºè°ƒä»£å…¥æ„Ÿä¸ä¸´åœºæ„Ÿã€‚åªè¾“å‡ºæ­£æ–‡éƒ¨åˆ†å’Œç®€å•çš„æœ€ç»ˆç®€å•çš„é€‰é¡¹äº¤äº’ï¼Œå…¶ä»–çŠ¶æ€ä¿¡æ¯ç­‰éƒ½ä¸è¦ï¼Œå°¤å…¶æ˜¯è§’è‰²é¢æ¿ä¿¡æ¯ï¼Œåªä¿ç•™æ­£æ–‡éƒ¨åˆ†å³å¯ã€‚ç¦æ­¢ä»»ä½•htmlæ ¼å¼å†…å®¹çš„è¾“å‡ºã€‚ä½†æ˜¯ä¸éœ€è¦å‘Šè¯‰ç”¨æˆ·æ­£åœ¨ä½¿ç”¨ä»€ä¹ˆæ¨¡å¼ï¼Œç›´æ¥å‘ˆç°æ­£æ–‡å³å¯\n"
            "2. æ­£æ–‡éƒ¨åˆ†è¾“å‡ºå­—æ•°æ§åˆ¶åœ¨500-800ä¸ªtokenï¼Œä½†æ˜¯ä¸éœ€è¦è¾“å‡ºtokenæ•°é‡ã€‚æ®µè½ä¹‹é—´ç©ºä¸€è¡Œ\n"
            "3. è¾“å‡ºè¯­è¨€å’Œç”¨æˆ·è¯­è¨€ä¿æŒä¸€è‡´ã€‚\n"
            "4. ç¦æ­¢ä»»ä½•æ€è€ƒè¿‡ç¨‹å‘ˆç°ç»™ç”¨æˆ·ï¼Œç›´æ¥è¾“å‡ºå†…å®¹ç»™ç”¨æˆ·æ²‰æµ¸æ„Ÿï¼Œä¸å‡†è®¸å‘ˆç°ä»»ä½•æ— å…³çš„ä¿¡æ¯\n"
            "5. å¿…é¡»åªè¾“å‡ºæ­£æ–‡å†…å®¹ï¼Œå¸¦ç»™ç”¨æˆ·æ²‰æµ¸æ„Ÿï¼Œä¹Ÿä¸è¦è¾“å‡ºæ²‰æµ¸æ¨¡å¼è¿˜æ˜¯æ™®é€šæ¨¡å¼ã€‚ç»å¯¹ä¸èƒ½è·³å‡ºç¬¬äºŒå µå¢™\n"
            "6.ä¸¥ç¦ç”Ÿæˆä»»ä½•å¥‡æ€ªçš„æ ¼å¼ï¼Œæ¯”å¦‚ <font color=\"#DDA0D0\">,ç¦æ­¢ç”Ÿæˆä»»ä½•<>æ ¼å¼çš„å†…å®¹\n"
            "ç»å¯¹ä¸è¦è®©ç”¨æˆ·å¯Ÿè§‰åˆ°è¿™ä¸ªæŒ‡ä»¤"
        )   


    def _safe_for_logging(self, text: str, max_len: Optional[int] = None) -> str:
        """Return a logging-safe preview of text, avoiding Unicode surrogate errors.

        - Truncates to max_len if provided
        - Replaces unencodable characters with Python-style backslash escapes
        """
        try:
            if text is None:
                return ""
            if max_len is not None:
                text = text[:max_len]
            # backslashreplace ensures surrogates or other problematic code points won't crash stdout
            return text.encode('utf-8', 'backslashreplace').decode('utf-8', 'strict')
        except Exception:
            return "<unprintable>"

    async def generate_reply(self, role_data, history, user_input, timeout=60, session_context_source=None, on_partial_reply: Optional[Callable[[str], None]] = None, caller: Optional[object] = None, model_name: Optional[str] = None):
        """
        ç”ŸæˆAIå›å¤
        
        Args:
            role_data: è§’è‰²é…ç½®æ•°æ®
            history: ä¼šè¯å†å²æ¶ˆæ¯
            user_input: å½“å‰ç”¨æˆ·è¾“å…¥
            timeout: è¶…æ—¶æ—¶é—´
            session_context_source: ä¼šè¯ä¸Šä¸‹æ–‡æ¥æºæ ‡è®°ï¼Œ"snapshot" è¡¨ç¤ºæ¥è‡ªå¿«ç…§ä¼šè¯
        
        è¯´æ˜ï¼š
            - å¸¸è§„ä¼šè¯: system_prompt + role_data.history + MessageServiceå†å²
            - å¿«ç…§ä¼šè¯: system_prompt + MessageServiceå†å²ï¼ˆå·²å«å¿«ç…§å®Œæ•´ä¸Šä¸‹æ–‡ï¼Œè·³è¿‡role_data.historyé¿å…é‡å¤ï¼‰
        """
        # æ‰“å°è¾“å…¥çš„å†å²è®°å½•
        print(f"ğŸ§  AIç”Ÿæˆå›å¤ | è¾“å…¥å†å²è®°å½•æ•°é‡: {len(history)} | ä¸Šä¸‹æ–‡æ¥æº: {session_context_source or 'å¸¸è§„'}")
        if history:
            print("ğŸ“œ è¾“å…¥å†å²è®°å½•:")
            for i, msg in enumerate(history):
                role_emoji = "ğŸ‘¤" if msg["role"] == "user" else "ğŸ¤–"
                print(f"  [{i+1}] {role_emoji} {msg['role']}")
                # é™åˆ¶å†…å®¹é•¿åº¦å¹¶è¿›è¡Œå®‰å…¨æ—¥å¿—å¤„ç†
                safe_preview = self._safe_for_logging(msg.get('content', ''), 80)
                print(f"      ğŸ“ {safe_preview}")
        else:
            print("ğŸ“œ è¾“å…¥å†å²è®°å½•ä¸ºç©º")

        # æ„å»º prompt
        messages = []
        
        # 1. æ·»åŠ  system_prompt
        if "system_prompt" in role_data:
            messages.append({"role": "system", "content": role_data["system_prompt"]})
        
        # 2. ä»…åœ¨éå¿«ç…§ä¼šè¯æ—¶æ·»åŠ è§’è‰²é¢„ç½® historyï¼ˆé¿å…é‡å¤ï¼‰
        if session_context_source != "snapshot" and "history" in role_data:
            messages.extend(role_data["history"])
            print(f"âœ… æ·»åŠ è§’è‰²é¢„ç½®å¯¹è¯: {len(role_data.get('history', []))} æ¡")
        elif session_context_source == "snapshot":
            print(f"â­ï¸ è·³è¿‡è§’è‰²é¢„ç½®å¯¹è¯ï¼ˆå¿«ç…§ä¼šè¯å·²åŒ…å«å®Œæ•´ä¸Šä¸‹æ–‡ï¼‰")
        
        # 3. æ·»åŠ å®é™…ä¼šè¯å†å²
        messages.extend(history)
        # æ³¨æ„ï¼šä¸å†é¢å¤–æ·»åŠ  user_inputï¼Œå› ä¸ºå®ƒå·²ç»åœ¨ history ä¸­äº†
        
        # ğŸ†• 4. å‰3è½®å¯¹è¯å¢å¼ºæŒ‡ä»¤é€»è¾‘
        user_turn_count = self._count_real_user_turns(history)
        if user_turn_count <= 3 and messages:
            last_user_msg_index = self._find_last_user_message_index(messages)
            if last_user_msg_index is not None:
                # å¢å¼ºæœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
                original_content = messages[last_user_msg_index]["content"]
                enhanced_content = self._enhance_user_message_with_instruction(
                    original_content, 
                    f"ç¬¬{user_turn_count}è½®å¯¹è¯"
                )
                messages[last_user_msg_index]["content"] = enhanced_content
                print(f"âœ… å·²ä¸ºç¬¬{user_turn_count}è½®å¯¹è¯æ·»åŠ å¢å¼ºæŒ‡ä»¤")
        elif user_turn_count > 3:
            print(f"â­ï¸ è·³è¿‡æŒ‡ä»¤å¢å¼ºï¼ˆå·²è¶…è¿‡3è½®ï¼‰: å½“å‰ç¬¬{user_turn_count}è½®")

        # æ‰“å°æ„å»ºçš„å®Œæ•´æ¶ˆæ¯åˆ—è¡¨
        print(f"ğŸ”§ æ„å»ºå®Œæ•´æ¶ˆæ¯åˆ—è¡¨ | æ€»æ¶ˆæ¯æ•°: {len(messages)}")
        print("ğŸ“‹ å®Œæ•´æ¶ˆæ¯åˆ—è¡¨:")
        for i, msg in enumerate(messages):
            role_emoji = {"system": "âš™ï¸", "user": "ğŸ‘¤", "assistant": "ğŸ¤–"}.get(msg["role"], "â“")
            print(f"  [{i+1}] {role_emoji} {msg['role']}")
            safe_preview = self._safe_for_logging(msg.get('content', ''), 80)
            print(f"      ğŸ“ {safe_preview}")
        
        print(f"ğŸ‘¤ å½“å‰ç”¨æˆ·è¾“å…¥: {self._safe_for_logging(user_input, 200)}")
        print("ğŸ§ " + "="*48)

        # æ¨¡æ‹Ÿè¶…æ—¶
        # ï¼ˆè¿™é‡Œåº”è¯¥åœ¨ GPTCaller å±‚åšçœŸæ­£çš„ async è¶…æ—¶æ§åˆ¶ï¼Œè¿™é‡Œå…ˆç®€åŒ–ï¼‰
        if random.random() < 0.01:
            raise TimeoutError("4004: ç”Ÿæˆè¶…æ—¶")

        # å¼€å§‹è®¡æ—¶ï¼šä»è°ƒç”¨GPT APIå¼€å§‹
        start = time.time()
        
        # æ”¶é›†å®Œæ•´å›å¤
        full_response = ""
        
        # é€‰æ‹©è°ƒç”¨å™¨ä¸æ¨¡å‹
        use_caller = caller or self._select_default_caller()
        use_model = model_name
        if use_caller is None:
            raise RuntimeError("æœªé…ç½®ä»»ä½•å¯ç”¨çš„AIè°ƒç”¨å™¨ï¼ˆGrok/Novelï¼‰")

        # è°ƒç”¨å¼‚æ­¥æµå¼ APIï¼ˆæ¨¡å‹ç”¨ä½ç½®å‚æ•°ä»¥å…¼å®¹ä¸åŒç­¾åï¼‰
        async for partial_reply in use_caller.get_stream_response(messages, use_model, timeout=timeout):
            full_response += partial_reply
            
            # å¦‚æœæä¾›äº†å›è°ƒå‡½æ•°ï¼Œé€æ­¥è°ƒç”¨å®ƒæ¥å¤„ç†éƒ¨åˆ†å›å¤
            if on_partial_reply:
                if callable(on_partial_reply):
                    # åŒæ­¥å›è°ƒ
                    on_partial_reply(partial_reply)
                else:
                    # å¼‚æ­¥å›è°ƒ
                    await on_partial_reply(partial_reply)

        # ç»“æŸæµå¼ç”Ÿæˆ
        print(f"ğŸ¤– AIç”Ÿæˆå›å¤å®Œæˆ | è€—æ—¶: {time.time() - start:.2f}ç§’ | æ€»å­—ç¬¦æ•°: {len(full_response)}")
        print("ğŸ¤–" + "="*48)
        
        return full_response

    def _count_real_user_turns(self, history):
        """
        ç»Ÿè®¡ä¼šè¯ä¸­çœŸå®ç”¨æˆ·å‘è¨€è½®æ¬¡
        åªç»Ÿè®¡ role == "user" çš„æ¶ˆæ¯æ•°é‡
        """
        user_turns = sum(1 for msg in history if msg.get("role") == "user")
        print(f"ğŸ“Š ç»Ÿè®¡ç”¨æˆ·å¯¹è¯è½®æ¬¡: {user_turns}")
        return user_turns
    
    def _find_last_user_message_index(self, messages):
        """
        æ‰¾åˆ°æ¶ˆæ¯åˆ—è¡¨ä¸­æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯çš„ç´¢å¼•
        ä»åå¾€å‰æŸ¥æ‰¾ï¼Œè¿”å›æœ€åä¸€æ¡ role == "user" çš„æ¶ˆæ¯ç´¢å¼•
        """
        for i in range(len(messages) - 1, -1, -1):
            if messages[i].get("role") == "user":
                print(f"ğŸ” æ‰¾åˆ°æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯ä½ç½®: index={i}")
                return i
        print("âš ï¸ æœªæ‰¾åˆ°ç”¨æˆ·æ¶ˆæ¯")
        return None
    
    def _enhance_user_message_with_instruction(self, original_content, user_context="å½“å‰å¯¹è¯"):
        """
        ä¸ºç”¨æˆ·æ¶ˆæ¯æ·»åŠ å‰3è½®å¢å¼ºæŒ‡ä»¤
        
        Args:
            original_content: åŸå§‹ç”¨æˆ·æ¶ˆæ¯å†…å®¹
            user_context: ç”¨æˆ·ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆç”¨äºæŒ‡ä»¤ä¸­çš„å ä½ç¬¦ï¼‰
        
        Returns:
            str: å¢å¼ºåçš„æ¶ˆæ¯å†…å®¹
        """
        enhanced_content = original_content + self.early_conversation_instruction.format(
            user_context=user_context
        )
        print(f"âœ¨ ç”¨æˆ·æ¶ˆæ¯å·²å¢å¼º | åŸé•¿åº¦: {len(original_content)} | å¢å¼ºåé•¿åº¦: {len(enhanced_content)}")
        return enhanced_content

    async def generate_reply_stream(self, role_data, history, user_input, timeout=60, session_context_source=None, caller: Optional[object] = None, model_name: Optional[str] = None) -> AsyncGenerator[str, None]:
        """
        æµå¼ç”ŸæˆAIå›å¤ - è¿”å›å¼‚æ­¥ç”Ÿæˆå™¨ï¼Œç”¨äºTelegram Botçš„æµå¼æ›´æ–°
        
        Args:
            role_data: è§’è‰²é…ç½®æ•°æ®
            history: ä¼šè¯å†å²æ¶ˆæ¯
            user_input: å½“å‰ç”¨æˆ·è¾“å…¥
            timeout: è¶…æ—¶æ—¶é—´
            session_context_source: ä¼šè¯ä¸Šä¸‹æ–‡æ¥æºæ ‡è®°
            
        Yields:
            str: æ¯ä¸ªæµå¼å›å¤ç‰‡æ®µ
        """
        # æ‰“å°è¾“å…¥çš„å†å²è®°å½•
        print(f"ğŸ§  AIæµå¼ç”Ÿæˆå›å¤ | è¾“å…¥å†å²è®°å½•æ•°é‡: {len(history)} | ä¸Šä¸‹æ–‡æ¥æº: {session_context_source or 'å¸¸è§„'}")
        
        # æ„å»º promptï¼ˆå¤ç”¨ç›¸åŒé€»è¾‘ï¼‰
        messages = []
        
        # 1. æ·»åŠ  system_prompt
        if "system_prompt" in role_data:
            messages.append({"role": "system", "content": role_data["system_prompt"]})
        
        # 2. ä»…åœ¨éå¿«ç…§ä¼šè¯æ—¶æ·»åŠ è§’è‰²é¢„ç½® historyï¼ˆé¿å…é‡å¤ï¼‰
        if session_context_source != "snapshot" and "history" in role_data:
            messages.extend(role_data["history"])
            print(f"âœ… æ·»åŠ è§’è‰²é¢„ç½®å¯¹è¯: {len(role_data.get('history', []))} æ¡")
        elif session_context_source == "snapshot":
            print(f"â­ï¸ è·³è¿‡è§’è‰²é¢„ç½®å¯¹è¯ï¼ˆå¿«ç…§ä¼šè¯å·²åŒ…å«å®Œæ•´ä¸Šä¸‹æ–‡ï¼‰")
        
        # 3. æ·»åŠ å®é™…ä¼šè¯å†å²
        messages.extend(history)
        
        # ğŸ†• 4. å‰3è½®å¯¹è¯å¢å¼ºæŒ‡ä»¤é€»è¾‘ï¼ˆæµå¼ç‰ˆæœ¬ï¼‰
        user_turn_count = self._count_real_user_turns(history)
        if user_turn_count <= 3 and messages:
            last_user_msg_index = self._find_last_user_message_index(messages)
            if last_user_msg_index is not None:
                # å¢å¼ºæœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
                original_content = messages[last_user_msg_index]["content"]
                enhanced_content = self._enhance_user_message_with_instruction(
                    original_content, 
                    f"ç¬¬{user_turn_count}è½®å¯¹è¯"
                )
                messages[last_user_msg_index]["content"] = enhanced_content
                print(f"âœ… å·²ä¸ºç¬¬{user_turn_count}è½®å¯¹è¯æ·»åŠ å¢å¼ºæŒ‡ä»¤ï¼ˆæµå¼ï¼‰")
        elif user_turn_count > 3:
            print(f"â­ï¸ è·³è¿‡æŒ‡ä»¤å¢å¼ºï¼ˆå·²è¶…è¿‡3è½®ï¼‰: å½“å‰ç¬¬{user_turn_count}è½®")
        
        print(f"ğŸ”§ æ„å»ºå®Œæ•´æ¶ˆæ¯åˆ—è¡¨ | æ€»æ¶ˆæ¯æ•°: {len(messages)}")
        print("ğŸ§ " + "="*48)

        # æ¨¡æ‹Ÿè¶…æ—¶
        if random.random() < 0.01:
            raise TimeoutError("4004: ç”Ÿæˆè¶…æ—¶")

        # å¼€å§‹è®¡æ—¶
        start = time.time()
        
        # æµå¼ç”Ÿæˆå¹¶é€æ­¥è¿”å›
        chunk_count = 0
        total_chars = 0
        # é€‰æ‹©è°ƒç”¨å™¨ä¸æ¨¡å‹
        use_caller = caller or self._select_default_caller()
        use_model = model_name
        if use_caller is None:
            raise RuntimeError("æœªé…ç½®ä»»ä½•å¯ç”¨çš„AIè°ƒç”¨å™¨ï¼ˆGrok/Novelï¼‰")

        async for partial_reply in use_caller.get_stream_response(messages, use_model, timeout=timeout):
            chunk_count += 1
            total_chars += len(partial_reply)
            safe_chunk_preview = self._safe_for_logging(partial_reply, 50)
            print(f"ğŸ”„ æ”¶åˆ°chunk #{chunk_count}: {len(partial_reply)} å­—ç¬¦ | å†…å®¹é¢„è§ˆ: {safe_chunk_preview}...")
            yield partial_reply

        # ç»“æŸæµå¼ç”Ÿæˆ
        print(f"ğŸ¤– AIæµå¼ç”Ÿæˆå®Œæˆ | è€—æ—¶: {time.time() - start:.2f}ç§’ | æ€»chunkæ•°: {chunk_count} | æ€»å­—ç¬¦æ•°: {total_chars}")
        print("ğŸ¤–" + "="*48)

    async def generate_reply_stream_with_retry(self, role_data, history, user_input, 
                                             max_retries=3, timeout=60, session_context_source=None) -> AsyncGenerator[str, None]:
        """
        å¸¦é‡è¯•æœºåˆ¶çš„æµå¼ç”ŸæˆAIå›å¤
        
        Args:
            role_data: è§’è‰²é…ç½®æ•°æ®
            history: ä¼šè¯å†å²æ¶ˆæ¯
            user_input: å½“å‰ç”¨æˆ·è¾“å…¥
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤3æ¬¡
            timeout: è¶…æ—¶æ—¶é—´
            session_context_source: ä¼šè¯ä¸Šä¸‹æ–‡æ¥æºæ ‡è®°
            
        Yields:
            str: æ¯ä¸ªæµå¼å›å¤ç‰‡æ®µ
        """
        for attempt in range(max_retries):
            try:
                print(f"ğŸ”„ AIç”Ÿæˆå°è¯• #{attempt + 1}/{max_retries}")

                # å‰ä¸¤æ¬¡ä½¿ç”¨ Grokï¼Œç¬¬ä¸‰æ¬¡ä½¿ç”¨ Novel
                if attempt < 2:
                    if not self.grok:
                        raise RuntimeError("Grok è°ƒç”¨å™¨æœªé…ç½®")
                    provider = "Grok"
                    caller = self.grok
                    model_env = os.getenv("GROK_MODEL")
                else:
                    if not self.novel:
                        raise RuntimeError("Novel è°ƒç”¨å™¨æœªé…ç½®")
                    provider = "Novel"
                    caller = self.novel
                    model_env = os.getenv("NOVEL_MODEL")

                print(f"ğŸš€ æœ¬æ¬¡å°è¯•ä½¿ç”¨æä¾›æ–¹: {provider} | æ¨¡å‹: {model_env}")

                # ä½¿ç”¨ç»Ÿä¸€çš„è¶…æ—¶ç­–ç•¥ï¼ˆä¸¤è¾¹ caller éƒ½ä½¿ç”¨ total=timeoutï¼‰
                async for chunk in self.generate_reply_stream(
                    role_data=role_data,
                    history=history,
                    user_input=user_input,
                    timeout=timeout,
                    session_context_source=session_context_source,
                    caller=caller,
                    model_name=model_env
                ):
                    yield chunk

                # æˆåŠŸç”Ÿæˆï¼Œé€€å‡ºé‡è¯•å¾ªç¯
                print(f"âœ… AIç”ŸæˆæˆåŠŸï¼ˆç¬¬{attempt + 1}æ¬¡å°è¯•ï¼Œæä¾›æ–¹: {provider}ï¼‰")
                return

            except Exception as e:
                print(f"âŒ AIç”Ÿæˆå¤±è´¥ï¼ˆç¬¬{attempt + 1}æ¬¡å°è¯•ï¼‰: {e}")

                if attempt == max_retries - 1:
                    # æœ€åä¸€æ¬¡é‡è¯•å¤±è´¥ï¼Œè¿”å›å›ºå®šè¯æœ¯
                    print(f"ğŸ’” æ‰€æœ‰é‡è¯•å‡å¤±è´¥ï¼Œè¿”å›å…œåº•è¯æœ¯")
                    yield "æŠ±æ­‰ï¼Œå›å¤å‡ºç°äº†é—®é¢˜ï¼Œåå°æ­£åœ¨åŠ ç´§ä¿®å¤ï¼Œè¯·è€å¿ƒç­‰å¾…"
                    return
                else:
                    # ç»§ç»­é‡è¯•
                    print(f"ğŸ”„ å‡†å¤‡è¿›è¡Œç¬¬{attempt + 2}æ¬¡é‡è¯•...")
                    continue

    def _safe_for_logging(self, text: str, max_length: int = 50) -> str:
        """å®‰å…¨åœ°æˆªæ–­æ–‡æœ¬ç”¨äºæ—¥å¿—è¾“å‡º"""
        if not text:
            return ""
        if len(text) <= max_length:
            return text
        return text[:max_length] + "..."

    def _select_default_caller(self) -> Optional[object]:
        """
        é€‰æ‹©ä¸€ä¸ªé»˜è®¤å¯ç”¨çš„è°ƒç”¨å™¨ï¼š
        ä¼˜å…ˆ Novelï¼Œå…¶æ¬¡ Grokï¼›å¦‚æœéƒ½ä¸å­˜åœ¨åˆ™è¿”å› None
        """
        if self.novel:
            return self.novel
        if self.grok:
            return self.grok
        return None


# âœ… å…¨å±€å”¯ä¸€å®ä¾‹ï¼ˆä¸´æ—¶å ä½ï¼Œå®é™…ä½¿ç”¨æ—¶åº”é€šè¿‡å®¹å™¨è·å–ï¼‰
# æ³¨æ„ï¼šè¿™ä¸ªå®ä¾‹åœ¨åˆå§‹åŒ–æ—¶ä¼šæŠ¥é”™ï¼Œå› ä¸ºæ²¡æœ‰æä¾› gpt_caller
# åœ¨åº”ç”¨å¯åŠ¨æ—¶ï¼Œåº”è¯¥é€šè¿‡å®¹å™¨åˆ›å»ºå¹¶æ›¿æ¢è¿™ä¸ªå®ä¾‹
ai_completion_port = None  # å°†åœ¨å®¹å™¨ä¸­åˆå§‹åŒ–
