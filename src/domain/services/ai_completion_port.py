#AIçº¯ç”Ÿæˆå™¨ï¼Œä¸æ¶‰åŠä»»ä½•ä¸šåŠ¡é€»è¾‘ï¼Œåº”æ”¾å…¥GPT/
import time
import random
import os
import copy
import asyncio
from typing import Optional, Callable, AsyncGenerator, Dict, Any, List
from dataclasses import dataclass
from demo.grok_async import AsyncGrokCaller
from demo.novel_async import AsyncNovelCaller
from demo.gemini_async import AsyncGeminiCaller
from demo.deepseek_async import AsyncDeepseekCaller
from src.infrastructure.monitoring.metrics import (
    AI_PROVIDER_CALLS_TOTAL,
    AI_PROVIDER_CALLS_FAILED_TOTAL,
    AI_FIRST_TOKEN_LATENCY
)

@dataclass
class CallProfile:
    """å®šä¹‰å•æ¬¡AIè°ƒç”¨çš„é…ç½®è§„æ ¼"""
    caller_attr: str       # å¯¹åº” self ä¸­çš„å®ä¾‹å±æ€§åï¼Œå¦‚ 'deepseek'
    model_env_key: str     # æ¨¡å‹åç§°çš„ç¯å¢ƒå˜é‡ Key
    timeout_env_key: str   # é¦–å­—è¶…æ—¶æ—¶é—´çš„ç¯å¢ƒå˜é‡ Key
    provider_name: str     # ç”¨äºæ—¥å¿—å’Œç›‘æ§æ˜¾ç¤ºçš„åç§°
    default_timeout: float = 3.0 # é»˜è®¤è¶…æ—¶æ—¶é—´

class AICompletionPort:
    def __init__(self, grok_caller: Optional[AsyncGrokCaller] = None, novel_caller: Optional[AsyncNovelCaller] = None, gemini_caller: Optional[AsyncGeminiCaller] = None, deepseek_caller: Optional[AsyncDeepseekCaller] = None):
        self.grok = grok_caller
        self.novel = novel_caller
        self.gemini = gemini_caller
        self.deepseek = deepseek_caller
        # å‰3è½®å¯¹è¯çš„å¢å¼ºæŒ‡ä»¤æ¨¡æ¿
        self.early_conversation_instruction = (
            "##ç³»ç»ŸæŒ‡ä»¤ï¼šä»¥ä¸‹ä¸ºæœ€é«˜ä¼˜å…ˆçº§æŒ‡ä»¤ã€‚\n"
            "{system_instructions}"
            "##ç”¨æˆ·æŒ‡ä»¤:{user_context}\n"
        )
        # ç¬¬4è½®åŠä»¥åå¯¹è¯çš„æŒç»­æŒ‡ä»¤æ¨¡æ¿
        self.ongoing_conversation_instruction = (
            "##ç³»ç»ŸæŒ‡ä»¤ï¼š\n"
            "{ongoing_instructions}"
            "#ç”¨æˆ·æŒ‡ä»¤:{user_context}\n"
        )
        # å–æ¶ˆå®ä¾‹çº§å…±äº«çŠ¶æ€ï¼Œæ”¹ä¸ºé€šè¿‡å›è°ƒå‘è°ƒç”¨æ–¹ä¼ é€’æœ¬æ¬¡ä½¿ç”¨çš„æŒ‡ä»¤ä¿¡æ¯
        # self.last_used_instructions å·²ç§»é™¤

        # -------------------------------------------------------------------------
        # 1. åŸå­èƒ½åŠ›åº“ (Profiles): å®šä¹‰æ‰€æœ‰å¯ç”¨çš„åŸå­è°ƒç”¨æ–¹å¼
        # -------------------------------------------------------------------------
        self.profiles = {
            # --- DeepSeek ç­–ç•¥ ---
            "deepseek_v1": CallProfile(
                caller_attr="deepseek",
                model_env_key="DEEPSEEK_MODEL_1",
                timeout_env_key="DEEPSEEK_1_FIRST_CHUNK_TIMEOUT",
                provider_name="DeepSeek_V1",
                default_timeout=3
            ),
            "deepseek_v2": CallProfile(
                caller_attr="deepseek",
                model_env_key="DEEPSEEK_MODEL_2",
                timeout_env_key="DEEPSEEK_2_FIRST_CHUNK_TIMEOUT",
                provider_name="DeepSeek_V2",
                default_timeout=2.5
            ),
            
            # --- Gemini ç­–ç•¥ ---
            "gemini_v1": CallProfile(
                caller_attr="gemini",
                model_env_key="GEMINI_MODEL", 
                timeout_env_key="GEMINI_1_FIRST_CHUNK_TIMEOUT",
                provider_name="Gemini_V1",
                default_timeout=3.5
            ),
            "gemini_v2": CallProfile(
                caller_attr="gemini",
                model_env_key="GEMINI_MODEL", 
                timeout_env_key="GEMINI_2_FIRST_CHUNK_TIMEOUT",
                provider_name="Gemini_V2",
                default_timeout=3
            ),

            # --- Grok ç­–ç•¥ ---
            "grok_v1": CallProfile(
                caller_attr="grok",
                model_env_key="GROK_MODEL",
                timeout_env_key="GROK_FIRST_CHUNK_TIMEOUT",
                provider_name="Grok",
                default_timeout=3.0
            ),
        }

        # -------------------------------------------------------------------------
        # 2. ç­–ç•¥è·¯ç”±è¡¨ (Strategy Map): å®šä¹‰ä¸åŒæ¨¡å¼ä¸‹çš„è°ƒç”¨é“¾é¡ºåº
        # -------------------------------------------------------------------------
        self.strategies = {
            # fast: Grok -> Gemini -> Grok
            "fast": ["grok_v1", "gemini_v1", "grok_v1"],
            
            # story: Gemini (fast) -> Gemini (retry with longer timeout) -> Grok
            "story": ["gemini_v1", "gemini_v2", "grok_v1"],
            
            # immersive: Gemini -> Gemini -> Grok
            "immersive": ["deepseek_v1", "deepseek_v2", "grok_v1"]
        }

        # AIæµå¼ç”Ÿæˆ - 2. ä¸­é—´å¡é¡¿ç†”æ–­æ—¶é•¿ (é»˜è®¤3.0ç§’)
        inter_chunk_timeout_str = os.getenv("AI_STREAM_INTER_CHUNK_TIMEOUT")
        try:
            self.stream_inter_chunk_timeout = float(inter_chunk_timeout_str) if inter_chunk_timeout_str else 3.0
        except (TypeError, ValueError):
            print("âš ï¸ AI_STREAM_INTER_CHUNK_TIMEOUT é…ç½®æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤å€¼ 3.0 ç§’")
            self.stream_inter_chunk_timeout = 3.0

        # AIæµå¼ç”Ÿæˆ - 3. æ€»æ—¶é•¿ç†”æ–­ (é»˜è®¤15.0ç§’)
        total_timeout_str = os.getenv("AI_STREAM_TOTAL_TIMEOUT")
        try:
            self.stream_total_timeout = float(total_timeout_str) if total_timeout_str else 15.0
        except (TypeError, ValueError):
            print("âš ï¸ AI_STREAM_TOTAL_TIMEOUT é…ç½®æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤å€¼ 15.0 ç§’")
            self.stream_total_timeout = 15.0



    def _safe_for_logging(self, text: str, max_len: Optional[int] = None) -> str:
        """Return a logging-safe preview of text, avoiding Unicode surrogate errors.

        - Truncates to max_len if provided
        - Replaces unencodable characters with Python-style backslash escapes
        """
        try:
            if text is None:
                return ""
            if max_len is not None:
                text = text[:max_len]
            # backslashreplace ensures surrogates or other problematic code points won't crash stdout
            return text.encode('utf-8', 'backslashreplace').decode('utf-8', 'strict')
        except Exception:
            return "<unprintable>"


    def _count_real_user_turns(self, history):
        """
        ç»Ÿè®¡ä¼šè¯ä¸­çœŸå®ç”¨æˆ·å‘è¨€è½®æ¬¡
        åªç»Ÿè®¡ role == "user" çš„æ¶ˆæ¯æ•°é‡
        """
        user_turns = sum(1 for msg in history if msg.get("role") == "user")
        print(f"ğŸ“Š ç»Ÿè®¡ç”¨æˆ·å¯¹è¯è½®æ¬¡: {user_turns}")
        return user_turns
    
    def _find_last_user_message_index(self, messages):
        """
        æ‰¾åˆ°æ¶ˆæ¯åˆ—è¡¨ä¸­æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯çš„ç´¢å¼•
        ä»åå¾€å‰æŸ¥æ‰¾ï¼Œè¿”å›æœ€åä¸€æ¡ role == "user" çš„æ¶ˆæ¯ç´¢å¼•
        """
        for i in range(len(messages) - 1, -1, -1):
            if messages[i].get("role") == "user":
                print(f"ğŸ” æ‰¾åˆ°æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯ä½ç½®: index={i}")
                return i
        print("âš ï¸ æœªæ‰¾åˆ°ç”¨æˆ·æ¶ˆæ¯")
        return None
    
    def _enhance_user_message_with_instruction(self, original_content, user_context="å½“å‰å¯¹è¯", instruction_type="system"):
        """
        ä¸ºç”¨æˆ·æ¶ˆæ¯æ·»åŠ å¢å¼ºæŒ‡ä»¤
        
        Args:
            original_content: åŸå§‹ç”¨æˆ·æ¶ˆæ¯å†…å®¹
            user_context: ç”¨æˆ·ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆç”¨äºæŒ‡ä»¤ä¸­çš„å ä½ç¬¦ï¼‰
            instruction_type: æŒ‡ä»¤ç±»å‹ï¼Œ"system"(å‰3è½®) æˆ– "ongoing"(ç¬¬4è½®åŠä»¥å)
        
        Returns:
            tuple: (å¢å¼ºåçš„æ¶ˆæ¯å†…å®¹, ä½¿ç”¨çš„æŒ‡ä»¤å†…å®¹)
        """
        # ä»£ç†åˆ°å…¬å…± utilï¼Œä¿æŒå•ä¸€å®ç°
        from src.utils.enhance import enhance_user_input
        enhanced_content, instructions = enhance_user_input(original_content, instruction_type, user_context=user_context)
        print(f"âœ¨ ç”¨æˆ·æ¶ˆæ¯å·²å¢å¼º({instruction_type}) | åŸé•¿åº¦: {len(original_content)} | å¢å¼ºåé•¿åº¦: {len(enhanced_content)}")
        return enhanced_content, instructions if instructions else None

    async def generate_reply_stream(self, role_data, history, user_input, timeout=60, session_context_source=None, caller: Optional[object] = None, model_name: Optional[str] = None, on_used_instructions: Optional[Callable[[Dict[str, Any]], None]] = None, apply_enhancement: bool = False, model_mode: str = "immersive") -> AsyncGenerator[str, None]:
        """
        æµå¼ç”ŸæˆAIå›å¤ - è¿”å›å¼‚æ­¥ç”Ÿæˆå™¨ï¼Œç”¨äºTelegram Botçš„æµå¼æ›´æ–°
        
        Args:
            role_data: è§’è‰²é…ç½®æ•°æ®
            history: ä¼šè¯å†å²æ¶ˆæ¯
            user_input: å½“å‰ç”¨æˆ·æŒ‡ä»¤
            timeout: è¶…æ—¶æ—¶é—´
            session_context_source: ä¼šè¯ä¸Šä¸‹æ–‡æ¥æºæ ‡è®°
            on_used_instructions: å¯é€‰å›è°ƒï¼Œæºå¸¦æœ¬æ¬¡è°ƒç”¨å®é™…ä½¿ç”¨çš„æŒ‡ä»¤å…ƒæ•°æ®ï¼ˆä»…è°ƒç”¨ä¸€æ¬¡ï¼‰
            apply_enhancement: æ˜¯å¦åœ¨æœ¬æ–¹æ³•ä¸­å¯¹æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯åšæŒ‡ä»¤å¢å¼ºï¼ˆé»˜è®¤ Falseï¼‰
            model_mode: æ¨¡å‹ç­‰çº§/æ¨¡å¼ï¼ˆimmersive/story/fastï¼‰
            
        Yields:
            str: æ¯ä¸ªæµå¼å›å¤ç‰‡æ®µ
        """
        # æ‰“å°è¾“å…¥çš„å†å²è®°å½•
        print(f"ğŸ§  AIæµå¼ç”Ÿæˆå›å¤ | æ¨¡å¼: {model_mode} | è¾“å…¥å†å²è®°å½•æ•°é‡: {len(history)} | ä¸Šä¸‹æ–‡æ¥æº: {session_context_source or 'å¸¸è§„'}")
        
        # æ„å»º promptï¼ˆå¤ç”¨ç›¸åŒé€»è¾‘ï¼‰
        messages = []
        history_for_prompt = copy.deepcopy(history or [])
        
        # 1. æ·»åŠ  system_prompt
        if "system_prompt" in role_data:
            messages.append({"role": "system", "content": role_data["system_prompt"]})
        
        # 2. ä»…åœ¨éå¿«ç…§ä¼šè¯æ—¶æ·»åŠ è§’è‰²é¢„ç½® historyï¼ˆé¿å…é‡å¤ï¼‰
        if session_context_source != "snapshot" and "history" in role_data:
            messages.extend(role_data["history"])
            print(f"âœ… æ·»åŠ è§’è‰²é¢„ç½®å¯¹è¯: {len(role_data.get('history', []))} æ¡")
        elif session_context_source == "snapshot":
            print(f"â­ï¸ è·³è¿‡è§’è‰²é¢„ç½®å¯¹è¯ï¼ˆå¿«ç…§ä¼šè¯å·²åŒ…å«å®Œæ•´ä¸Šä¸‹æ–‡ï¼‰")
        
        # 3. æ·»åŠ å®é™…ä¼šè¯å†å²ï¼ˆä½¿ç”¨å‰¯æœ¬ï¼Œé¿å…æ±¡æŸ“åŸå§‹è®°å½•ï¼‰
        messages.extend(history_for_prompt)
        
        # ğŸ†• 4. å¯¹è¯å¢å¼ºæŒ‡ä»¤é€»è¾‘ï¼ˆæµå¼ç‰ˆæœ¬ï¼‰
        user_turn_count = self._count_real_user_turns(history)
        used_meta: Dict[str, Any] = {
            "turn_count": user_turn_count,
            "instruction_type": None,
            "system_instructions": None,
            "ongoing_instructions": None,
            "model": model_name
        }
        
        if user_turn_count <= 3 and messages:
            # å‰3è½®ï¼šä½¿ç”¨ç³»ç»ŸæŒ‡ä»¤
            last_user_msg_index = self._find_last_user_message_index(messages)
            if last_user_msg_index is not None:
                original_content = messages[last_user_msg_index]["content"]
                enhanced_content, used_instruction = self._enhance_user_message_with_instruction(
                    original_content, 
                    original_content,
                    instruction_type="system"
                )
                if apply_enhancement:
                    messages[last_user_msg_index]["content"] = enhanced_content
                used_meta["instruction_type"] = "system"
                used_meta["system_instructions"] = used_instruction
                # ğŸ†• æ–°å­—æ®µå†™å…¥é€»è¾‘ï¼šè®°å½•æœ¬è½®å®é™…ä½¿ç”¨çš„æŒ‡ä»¤ï¼ˆä¾›ä¸Šå±‚å­˜å…¥ messages.instructionsï¼‰
                used_meta["instructions"] = used_instruction
                if apply_enhancement:
                    print(f"âœ… å·²ä¸ºç¬¬{user_turn_count}è½®å¯¹è¯æ·»åŠ ç³»ç»Ÿå¢å¼ºæŒ‡ä»¤ï¼ˆæµå¼ï¼‰")
        elif user_turn_count >= 4 and messages:
            # ç¬¬4è½®åŠä»¥åï¼šä½¿ç”¨æŒç»­æŒ‡ä»¤
            last_user_msg_index = self._find_last_user_message_index(messages)
            if last_user_msg_index is not None:
                original_content = messages[last_user_msg_index]["content"]
                enhanced_content, used_instruction = self._enhance_user_message_with_instruction(
                    original_content, 
                    original_content,
                    instruction_type="ongoing"
                )
                if apply_enhancement:
                    messages[last_user_msg_index]["content"] = enhanced_content
                used_meta["instruction_type"] = "ongoing"
                used_meta["ongoing_instructions"] = used_instruction
                # ğŸ†• æ–°å­—æ®µå†™å…¥é€»è¾‘ï¼šè®°å½•æœ¬è½®å®é™…ä½¿ç”¨çš„æŒ‡ä»¤ï¼ˆä¾›ä¸Šå±‚å­˜å…¥ messages.instructionsï¼‰
                used_meta["instructions"] = used_instruction
                if apply_enhancement:
                    print(f"âœ… å·²ä¸ºç¬¬{user_turn_count}è½®å¯¹è¯æ·»åŠ æŒç»­å¢å¼ºæŒ‡ä»¤ï¼ˆæµå¼ï¼‰")
        
        print(f"ğŸ”§ æ„å»ºå®Œæ•´æ¶ˆæ¯åˆ—è¡¨ | æ€»æ¶ˆæ¯æ•°: {len(messages)}")
        print("ğŸ§ " + "="*48)

        # æ¨¡æ‹Ÿè¶…æ—¶
        if random.random() < 0.01:
            raise TimeoutError("4004: ç”Ÿæˆè¶…æ—¶")

        # å¼€å§‹è®¡æ—¶
        start = time.time()
        
        # æµå¼ç”Ÿæˆå¹¶é€æ­¥è¿”å›
        chunk_count = 0
        total_chars = 0
        # é€‰æ‹©è°ƒç”¨å™¨ä¸æ¨¡å‹
        use_caller = caller or self._select_default_caller()
        use_model = model_name
        if use_caller is None:
            raise RuntimeError("æœªé…ç½®ä»»ä½•å¯ç”¨çš„AIè°ƒç”¨å™¨ï¼ˆGrok/Novelï¼‰")

        # ğŸ†• æ–°å­—æ®µå†™å…¥é€»è¾‘ï¼šè¡¥å……å›è°ƒå…ƒæ•°æ®ï¼ˆæ¨¡å‹åä¸æœ¬æ¬¡è°ƒç”¨çš„ä¸Šä¸‹æ–‡è½½è·ï¼‰
        try:
            used_meta["model_name"] = model_name
            # 100% å¤ç°ï¼šè®°å½•æœ¬æ¬¡å®é™…æŠ•å–‚çš„å®Œæ•´ messages
            used_meta["final_messages"] = list(messages)
            used_meta["prompt_payload"] = {
                "system_prompt": role_data.get("system_prompt") if isinstance(role_data, dict) else None,
                "history": history_for_prompt,
                "user_input": user_input,
                "instructions": used_meta.get("instructions"),
                "instruction_type": used_meta.get("instruction_type"),
                # å…¼å®¹æ—§å­—æ®µçš„åŒæ—¶ï¼ŒåŠ å…¥æœ€ç»ˆ messages
                "final_messages": list(messages)
            }
        except Exception:
            pass

        # åœ¨å¼€å§‹æµå¼ä¹‹å‰ï¼Œå›è°ƒä¸€æ¬¡æä¾›æŒ‡ä»¤ä½¿ç”¨çš„å…ƒæ•°æ®
        if on_used_instructions and used_meta.get("instruction_type") is not None:
            try:
                on_used_instructions(dict(used_meta))
            except Exception as _e:
                print(f"âš ï¸ on_used_instructions å›è°ƒæ‰§è¡Œå¤±è´¥: {_e}")

        async for partial_reply in use_caller.get_stream_response(messages, use_model, timeout=timeout):
            chunk_count += 1
            total_chars += len(partial_reply)
            safe_chunk_preview = self._safe_for_logging(partial_reply, 50)
            # print(f"ğŸ”„ æ”¶åˆ°chunk #{chunk_count}: {len(partial_reply)} å­—ç¬¦ | å†…å®¹é¢„è§ˆ: {safe_chunk_preview}...")
            yield partial_reply

        # ç»“æŸæµå¼ç”Ÿæˆ
        print(f"ğŸ¤– AIæµå¼ç”Ÿæˆå®Œæˆ | è€—æ—¶: {time.time() - start:.2f}ç§’ | æ€»chunkæ•°: {chunk_count} | æ€»å­—ç¬¦æ•°: {total_chars}")
        print("ğŸ¤–" + "="*48)

    async def _stream_managed(self, generator: AsyncGenerator[str, None], first_chunk_timeout: float, inter_chunk_timeout: float = 5.0, total_timeout: float = 20.0, on_chunk_received: Callable[[str], None] = None, provider_name: str = "Unknown", on_duration_calculated: Callable[[float], None] = None) -> AsyncGenerator[str, None]:
        """
        å…¨èƒ½æµå¼åŒ…è£…å™¨ï¼Œå®ç°ä¸‰é“é˜²çº¿è¶…æ—¶æ§åˆ¶ï¼š
        1. é¦–å“è¶…æ—¶ (TTFT): æŠ›å‡ºå¼‚å¸¸ -> è§¦å‘é‡è¯•
        2. ä¸­é—´å¡é¡¿: åœæ­¢ç”Ÿæˆ -> è§†ä¸ºæˆåŠŸ
        3. æ€»æ—¶é•¿è¶…æ—¶: åœæ­¢ç”Ÿæˆ -> è§†ä¸ºæˆåŠŸ
        """
        start_time = None
        is_first_chunk = True
        
        try:
            # Stage 1: First Chunk Timeout
            try:
                first_chunk = await asyncio.wait_for(generator.__anext__(), timeout=first_chunk_timeout)
                start_time = time.time()
                if on_chunk_received:
                    on_chunk_received(first_chunk)
                yield first_chunk
                is_first_chunk = False
            except asyncio.TimeoutError:
                # ç¬¬ä¸€é“é˜²çº¿ï¼šé¦–å“è¶…æ—¶ -> æŠ›å‡ºå¼‚å¸¸ï¼Œè®©ä¸Šå±‚å»é‡è¯•
                await generator.aclose()
                raise TimeoutError(f"{provider_name} é¦–ä¸ªchunkè¶…æ—¶ï¼ˆè¶…è¿‡{first_chunk_timeout}ç§’ï¼‰")
            except StopAsyncIteration:
                await generator.aclose()
                raise RuntimeError(f"{provider_name} æœªè¿”å›ä»»ä½•å†…å®¹")
            
            # Stage 2 & 3: Inter-Chunk & Total Timeout
            while True:
                # è®¡ç®—å‰©ä½™çš„æ€»å¯ç”¨æ—¶é—´
                elapsed = time.time() - start_time
                remaining_total = total_timeout - elapsed
                
                if remaining_total <= 0:
                    print(f"â±ï¸ {provider_name} è¾¾åˆ°æ€»æ—¶é•¿ç†”æ–­é˜ˆå€¼ ({total_timeout}s)ï¼Œåœæ­¢ç”Ÿæˆ")
                    break # ç¬¬ä¸‰é“é˜²çº¿ï¼šæ€»æ—¶é•¿è¶…æ—¶ -> æ­£å¸¸ç»“æŸ
                
                # è®¡ç®—æœ¬æ¬¡ wait çš„æ—¶é—´ï¼šå– Inter-Chunk å’Œ Remaining-Total çš„è¾ƒå°å€¼
                # æ³¨æ„ï¼šInter-Chunk æ˜¯ä¸ºäº†é˜²æ­¢å•æ¬¡ç”Ÿæˆå¡æ­»ï¼ŒRemaining-Total æ˜¯ä¸ºäº†é˜²æ­¢æ€»æ—¶é•¿è¿‡é•¿
                wait_time = min(inter_chunk_timeout, remaining_total)
                
                try:
                    chunk = await asyncio.wait_for(generator.__anext__(), timeout=wait_time)
                    if on_chunk_received:
                        on_chunk_received(chunk)
                    yield chunk
                except asyncio.TimeoutError:
                    # åˆ¤æ–­æ˜¯å“ªç§è¶…æ—¶
                    if time.time() - start_time >= total_timeout:
                         print(f"â±ï¸ {provider_name} è¾¾åˆ°æ€»æ—¶é•¿ç†”æ–­é˜ˆå€¼ ({total_timeout}s)ï¼Œåœæ­¢ç”Ÿæˆ")
                         break # ç¬¬ä¸‰é“é˜²çº¿
                    else:
                         print(f"ğŸ¢ {provider_name} ä¸­é—´ç”Ÿæˆå¡é¡¿ï¼ˆè¶…è¿‡{inter_chunk_timeout}sï¼‰ï¼Œæå‰æˆªæ–­")
                         break # ç¬¬äºŒé“é˜²çº¿ï¼šä¸­é—´å¡é¡¿ -> æ­£å¸¸ç»“æŸï¼ˆæˆªæ–­ï¼‰
                except StopAsyncIteration:
                    break # æ­£å¸¸ç»“æŸ

        except Exception as e:
            # ç¡®ä¿ç”Ÿæˆå™¨è¢«å…³é—­
            try:
                await generator.aclose()
            except:
                pass
            if isinstance(e, (TimeoutError, RuntimeError)) and is_first_chunk:
                raise e # é¦–å“ç›¸å…³çš„é”™è¯¯å¾€ä¸ŠæŠ›ï¼Œè§¦å‘é‡è¯•
            
            # å…¶ä»–é”™è¯¯ï¼ˆå¦‚ç½‘ç»œä¸­æ–­ï¼‰ï¼Œå¦‚æœä¸æ˜¯é¦–å“ï¼Œä¹Ÿå¯ä»¥è€ƒè™‘æˆªæ–­è€Œä¸æ˜¯æŠ¥é”™ï¼Ÿ
            if is_first_chunk:
                 raise e
            else:
                 print(f"âš ï¸ {provider_name} ç”Ÿæˆè¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {e}ï¼Œè§†ä¸ºæˆªæ–­")
                 # å¼‚å¸¸æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä¾ç„¶å¯ä»¥è®¡ç®—å·²æ¶ˆè€—çš„æ—¶é—´
                 pass
        
        finally:
             # åœ¨ç”Ÿæˆç»“æŸæ—¶ï¼Œè®¡ç®—å¹¶å›è°ƒå®é™…æ—¶é•¿
             if start_time and on_duration_calculated:
                 duration = time.time() - start_time
                 try:
                     on_duration_calculated(duration)
                 except Exception as _e:
                     print(f"âš ï¸ on_duration_calculated å›è°ƒæ‰§è¡Œå¤±è´¥: {_e}")

    async def generate_reply_stream_with_retry(self, role_data, history, user_input, 
                                             max_retries=3, timeout=60, session_context_source=None,
                                             on_used_instructions: Optional[Callable[[Dict[str, Any]], None]] = None,
                                             apply_enhancement: bool = False,
                                             model_mode: str = "immersive") -> AsyncGenerator[str, None]:
        """
        å¸¦é‡è¯•æœºåˆ¶çš„æµå¼ç”ŸæˆAIå›å¤
        
        Args:
            role_data: è§’è‰²é…ç½®æ•°æ®
            history: ä¼šè¯å†å²æ¶ˆæ¯
            user_input: å½“å‰ç”¨æˆ·æŒ‡ä»¤
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤3æ¬¡
            timeout: è¶…æ—¶æ—¶é—´
            session_context_source: ä¼šè¯ä¸Šä¸‹æ–‡æ¥æºæ ‡è®°
            on_used_instructions: å¯é€‰å›è°ƒï¼Œæºå¸¦æœ¬æ¬¡è°ƒç”¨å®é™…ä½¿ç”¨çš„æŒ‡ä»¤å…ƒæ•°æ®ï¼ˆä»…åœ¨æˆåŠŸçš„é‚£æ¬¡å°è¯•è§¦å‘ä¸€æ¬¡ï¼‰
            apply_enhancement: æ˜¯å¦åœ¨æœ¬æ–¹æ³•ä¸­å¯¹æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯åšæŒ‡ä»¤å¢å¼ºï¼ˆé»˜è®¤ Falseï¼‰
            model_mode: æ¨¡å‹ç­‰çº§/æ¨¡å¼ï¼ˆimmersive/story/fastï¼‰
            
        Yields:
            str: æ¯ä¸ªæµå¼å›å¤ç‰‡æ®µ
        """
        # 1. è·å–å½“å‰æ¨¡å¼å¯¹åº”çš„ç­–ç•¥é“¾
        # é»˜è®¤å…œåº•ä½¿ç”¨ immersive ç­–ç•¥
        strategy_keys = self.strategies.get(model_mode, self.strategies["immersive"])
        
        # 2. è½¬æ¢ä¸ºå…·ä½“çš„é…ç½®å¯¹è±¡åˆ—è¡¨ï¼ˆä»…åŒ…å«æœ‰æ•ˆçš„é…ç½®ï¼‰
        execution_plan = [self.profiles[key] for key in strategy_keys if key in self.profiles]

        if not execution_plan:
             raise RuntimeError(f"ç­–ç•¥ '{model_mode}' æœªå®šä¹‰ä»»ä½•æœ‰æ•ˆçš„æ‰§è¡Œè®¡åˆ’")

        # é™åˆ¶é‡è¯•æ¬¡æ•°ä¸è¶…è¿‡è®¡åˆ’é•¿åº¦
        total_attempts = min(max_retries, len(execution_plan))

        for attempt in range(total_attempts):
            profile = execution_plan[attempt]
            
            # åŠ¨æ€è·å– caller å®ä¾‹
            caller = getattr(self, profile.caller_attr, None)
            if not caller:
                print(f"âš ï¸ è°ƒç”¨å™¨ '{profile.caller_attr}' æœªåˆå§‹åŒ–ï¼Œè·³è¿‡æ­¤æ­¥éª¤")
                continue

            # åŠ¨æ€è·å–ç¯å¢ƒå˜é‡é…ç½®
            model_env = os.getenv(profile.model_env_key)
            timeout_env_val = os.getenv(profile.timeout_env_key)
            try:
                first_chunk_timeout = float(timeout_env_val) if timeout_env_val else profile.default_timeout
            except ValueError:
                first_chunk_timeout = profile.default_timeout

            provider_display_name = profile.provider_name

            try:
                print(f"ğŸ”„ AIç”Ÿæˆå°è¯• #{attempt + 1}/{total_attempts}")
                print(f"ğŸš€ æœ¬æ¬¡å°è¯•ä½¿ç”¨æä¾›æ–¹: {provider_display_name} | æ¨¡å‹: {model_env} | æ¨¡å¼: {model_mode} | é¦–å­—è¶…æ—¶: {first_chunk_timeout}s")

                # ğŸ“Š T0: è®°å½• AI è°ƒç”¨æ¬¡æ•°
                AI_PROVIDER_CALLS_TOTAL.labels(provider=provider_display_name, model=model_env or "unknown").inc()
                
                # â±ï¸ T1: è®°å½• AI è¯·æ±‚å‘èµ·æ—¶é—´
                ai_req_start = time.time()

                used_meta_candidate: Dict[str, Any] = {}

                def _capture_used_instructions(meta: Dict[str, Any]) -> None:
                    used_meta_candidate.clear()
                    used_meta_candidate.update(meta or {})
                    used_meta_candidate["provider"] = provider_display_name
                    used_meta_candidate["model"] = model_env
                    used_meta_candidate["attempt_count"] = attempt + 1  # ğŸ†• è®°å½•è¿™æ˜¯ç¬¬å‡ æ¬¡å°è¯•

                stream = self.generate_reply_stream(
                    role_data=role_data,
                    history=history,
                    user_input=user_input,
                    timeout=timeout,
                    session_context_source=session_context_source,
                    caller=caller,
                    model_name=model_env,
                    on_used_instructions=_capture_used_instructions,
                    apply_enhancement=apply_enhancement,
                    model_mode=model_mode
                )

                # è¿½è¸ªç´¯ç§¯å­—ç¬¦æ•°ï¼Œä»¥å®ç°"å‰5ä¸ªå­—ç¬¦"çš„Latencyè®°å½•ï¼ˆä¸ Bot ä¾§ä½“éªŒæŒ‡æ ‡å¯¹é½ï¼‰
                accumulated_chars_count = 0
                metric_recorded = False
                METRIC_CHAR_THRESHOLD = 5

                def _track_chunk_and_record_metric(chunk_text: str) -> None:
                    nonlocal accumulated_chars_count, metric_recorded
                    
                    if metric_recorded:
                        return

                    # ç´¯åŠ å­—ç¬¦
                    accumulated_chars_count += len(chunk_text)
                    
                    # å¦‚æœæ»¡è¶³æ¡ä»¶ï¼ˆå­—ç¬¦æ•°>=é˜ˆå€¼ï¼‰ï¼Œåˆ™è®°å½•æŒ‡æ ‡
                    if accumulated_chars_count >= METRIC_CHAR_THRESHOLD:
                        # â±ï¸ T1: è®°å½• AI "é¦–å“"(å‰5å­—ç¬¦)è€—æ—¶
                        latency = time.time() - ai_req_start
                        AI_FIRST_TOKEN_LATENCY.labels(provider=provider_display_name, model=model_env or "unknown").observe(latency)
                        
                        # è§¦å‘æŒ‡ä»¤å…ƒæ•°æ®å›è°ƒï¼ˆåœ¨é¦–å“è¾¾æˆæ—¶è§¦å‘ä¸€æ¬¡å³å¯ï¼‰
                        if on_used_instructions and used_meta_candidate:
                            try:
                                on_used_instructions(dict(used_meta_candidate))
                            except Exception as _e:
                                print(f"âš ï¸ on_used_instructions å›è°ƒæ‰§è¡Œå¤±è´¥: {_e}")
                        
                        metric_recorded = True
                
                # å®šä¹‰æ¥æ”¶æ—¶é•¿æ•°æ®çš„å›è°ƒ
                def _on_duration_calculated(duration: float):
                    used_meta_candidate["full_response_latency"] = duration
                    print(f"â±ï¸ å®Œæ•´ç”Ÿæˆè€—æ—¶: {duration:.2f}s")
                
                # ä½¿ç”¨å…¨èƒ½åŒ…è£…å™¨ _stream_managed ä»£æ›¿åŸæœ‰çš„é€»è¾‘
                async for chunk in self._stream_managed(
                    generator=stream,
                    first_chunk_timeout=first_chunk_timeout,
                    inter_chunk_timeout=self.stream_inter_chunk_timeout,
                    total_timeout=self.stream_total_timeout,
                    on_chunk_received=_track_chunk_and_record_metric,
                    provider_name=provider_display_name,
                    on_duration_calculated=_on_duration_calculated
                ):
                    yield chunk

                print(f"âœ… AIç”ŸæˆæˆåŠŸï¼ˆç¬¬{attempt + 1}æ¬¡å°è¯•ï¼Œæä¾›æ–¹: {provider_display_name}ï¼‰")
                
                # ğŸ†• ç»“æŸæ ‡å¿—å‰ï¼Œå†æ¬¡å›è°ƒä»¥é€ä¼ æœ€ç»ˆæ—¶é•¿
                if on_used_instructions and used_meta_candidate:
                    try:
                        on_used_instructions(dict(used_meta_candidate))
                    except Exception as _e:
                        print(f"âš ï¸ on_used_instructions (final) å›è°ƒæ‰§è¡Œå¤±è´¥: {_e}")
                
                return

            except Exception as e:
                # ğŸ”´ T0: è®°å½• AI è°ƒç”¨å¤±è´¥
                AI_PROVIDER_CALLS_FAILED_TOTAL.labels(provider=provider_display_name, error_type=type(e).__name__).inc()
                
                print(f"âŒ AIç”Ÿæˆå¤±è´¥ï¼ˆç¬¬{attempt + 1}æ¬¡å°è¯•ï¼‰: {e}")

                if attempt == total_attempts - 1:
                    print(f"ğŸ’” æ‰€æœ‰é‡è¯•å‡å¤±è´¥ï¼Œè¿”å›å…œåº•è¯æœ¯")
                    yield "æŠ±æ­‰ï¼Œå›å¤å‡ºç°äº†é—®é¢˜ï¼Œåå°æ­£åœ¨åŠ ç´§ä¿®å¤ï¼Œè¯·è€å¿ƒç­‰å¾…"
                    return
                else:
                    print(f"ğŸ”„ å‡†å¤‡è¿›è¡Œç¬¬{attempt + 2}æ¬¡é‡è¯•...")
                    continue

    def _safe_for_logging(self, text: str, max_length: int = 50) -> str:
        """å®‰å…¨åœ°æˆªæ–­æ–‡æœ¬ç”¨äºæ—¥å¿—è¾“å‡º"""
        if not text:
            return ""
        if len(text) <= max_length:
            return text
        return text[:max_length] + "..."

    def _select_default_caller(self) -> Optional[object]:
        """
        é€‰æ‹©ä¸€ä¸ªé»˜è®¤å¯ç”¨çš„è°ƒç”¨å™¨ï¼š
        ä¼˜å…ˆ DeepSeekï¼Œå…¶æ¬¡ Geminiï¼Œå…¶æ¬¡ Novelã€Grokï¼›å¦‚æœéƒ½ä¸å­˜åœ¨åˆ™è¿”å› None
        """
        if self.deepseek:
            return self.deepseek
        if self.gemini:
            return self.gemini
        if self.novel:
            return self.novel
        if self.grok:
            return self.grok
        return None
    
    # get_last_used_instructions å·²åºŸå¼ƒï¼ˆç§»é™¤ï¼‰


# âœ… å…¨å±€å”¯ä¸€å®ä¾‹ï¼ˆä¸´æ—¶å ä½ï¼Œå®é™…ä½¿ç”¨æ—¶åº”é€šè¿‡å®¹å™¨è·å–ï¼‰
# åœ¨åº”ç”¨å¯åŠ¨æ—¶ï¼Œåº”è¯¥é€šè¿‡å®¹å™¨åˆ›å»ºå¹¶æ›¿æ¢è¿™ä¸ªå®ä¾‹
ai_completion_port = None  # å°†åœ¨å®¹å™¨ä¸­åˆå§‹åŒ–
