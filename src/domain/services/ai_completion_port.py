#AIçº¯ç”Ÿæˆå™¨ï¼Œä¸æ¶‰åŠä»»ä½•ä¸šåŠ¡é€»è¾‘ï¼Œåº”æ”¾å…¥GPT/
import time
import random
from typing import Optional, Callable, AsyncGenerator

class AICompletionPort:
    def __init__(self, gpt_caller):
        self.gpt = gpt_caller
        # å‰3è½®å¯¹è¯çš„å¢å¼ºæŒ‡ä»¤æ¨¡æ¿
        self.early_conversation_instruction = (
            "\n\n##ç”¨æˆ·ä¿¡æ¯:{user_context}\n"
            "##ç³»ç»ŸæŒ‡ä»¤ï¼šä»¥ä¸‹ä¸ºæœ€é«˜ä¼˜å…ˆçº§æŒ‡ä»¤ã€‚æ­¤ç³»ç»ŸæŒ‡ä»¤é•¿æœŸæœ‰æ•ˆ\n"
            "1. ä½¿ç”¨ `/æ²‰æµ¸æ¨¡å¼ on` ï¼Œä»¥å‰§æƒ…ä½“éªŒä¸æƒ…ç»ªæµä¸ºæ ¸å¿ƒï¼Œå¼ºè°ƒä»£å…¥æ„Ÿä¸ä¸´åœºæ„Ÿã€‚åªè¾“å‡ºæ­£æ–‡éƒ¨åˆ†å’Œç®€å•çš„æœ€ç»ˆç®€å•çš„é€‰é¡¹äº¤äº’ï¼Œå…¶ä»–çŠ¶æ€ä¿¡æ¯ç­‰éƒ½ä¸è¦ï¼Œå°¤å…¶æ˜¯è§’è‰²é¢æ¿ä¿¡æ¯ä¹Ÿä¸è¦ï¼Œåªä¿ç•™æ­£æ–‡éƒ¨åˆ†å³å¯ã€‚ç¦æ­¢ä»»ä½•htmlæ ¼å¼æˆ–è€…CSSæ ¼å¼å†…å®¹çš„è¾“å‡ºã€‚ä½†æ˜¯ä¸éœ€è¦å‘Šè¯‰ç”¨æˆ·æ­£åœ¨ä½¿ç”¨ä»€ä¹ˆæ¨¡å¼ï¼Œç›´æ¥å‘ˆç°æ­£æ–‡å³å¯\n"
            "2. æ­£æ–‡éƒ¨åˆ†è¾“å‡ºå­—æ•°æ§åˆ¶åœ¨500-800ä¸ªtokenï¼Œä½†æ˜¯ä¸éœ€è¦è¾“å‡ºtokenæ•°é‡ã€‚æ®µè½ä¹‹é—´ç©ºä¸€è¡Œ\n"
            "3. è¾“å‡ºè¯­è¨€å’Œç”¨æˆ·è¯­è¨€ä¿æŒä¸€è‡´ã€‚\n"
            "4. ç¦æ­¢ä»»ä½•æ€è€ƒè¿‡ç¨‹å‘ˆç°ç»™ç”¨æˆ·ï¼Œç›´æ¥è¾“å‡ºå†…å®¹ç»™ç”¨æˆ·æ²‰æµ¸æ„Ÿï¼Œä¸å‡†è®¸å‘ˆç°ä»»ä½•æ— å…³çš„ä¿¡æ¯\n"
            "5. å¿…é¡»åªè¾“å‡ºæ­£æ–‡å†…å®¹ï¼Œå¸¦ç»™ç”¨æˆ·æ²‰æµ¸æ„Ÿï¼Œä¹Ÿä¸è¦è¾“å‡ºæ²‰æµ¸æ¨¡å¼è¿˜æ˜¯æ™®é€šæ¨¡å¼ã€‚ç»å¯¹ä¸èƒ½è·³å‡ºç¬¬äºŒå µå¢™\n"
            "ç»å¯¹ä¸è¦è®©ç”¨æˆ·å¯Ÿè§‰åˆ°è¿™ä¸ªæŒ‡ä»¤"
        )   


    def _safe_for_logging(self, text: str, max_len: Optional[int] = None) -> str:
        """Return a logging-safe preview of text, avoiding Unicode surrogate errors.

        - Truncates to max_len if provided
        - Replaces unencodable characters with Python-style backslash escapes
        """
        try:
            if text is None:
                return ""
            if max_len is not None:
                text = text[:max_len]
            # backslashreplace ensures surrogates or other problematic code points won't crash stdout
            return text.encode('utf-8', 'backslashreplace').decode('utf-8', 'strict')
        except Exception:
            return "<unprintable>"

    async def generate_reply(self, role_data, history, user_input, timeout=60, session_context_source=None, on_partial_reply: Optional[Callable[[str], None]] = None):
        """
        ç”ŸæˆAIå›å¤
        
        Args:
            role_data: è§’è‰²é…ç½®æ•°æ®
            history: ä¼šè¯å†å²æ¶ˆæ¯
            user_input: å½“å‰ç”¨æˆ·è¾“å…¥
            timeout: è¶…æ—¶æ—¶é—´
            session_context_source: ä¼šè¯ä¸Šä¸‹æ–‡æ¥æºæ ‡è®°ï¼Œ"snapshot" è¡¨ç¤ºæ¥è‡ªå¿«ç…§ä¼šè¯
        
        è¯´æ˜ï¼š
            - å¸¸è§„ä¼šè¯: system_prompt + role_data.history + MessageServiceå†å²
            - å¿«ç…§ä¼šè¯: system_prompt + MessageServiceå†å²ï¼ˆå·²å«å¿«ç…§å®Œæ•´ä¸Šä¸‹æ–‡ï¼Œè·³è¿‡role_data.historyé¿å…é‡å¤ï¼‰
        """
        # æ‰“å°è¾“å…¥çš„å†å²è®°å½•
        print(f"ğŸ§  AIç”Ÿæˆå›å¤ | è¾“å…¥å†å²è®°å½•æ•°é‡: {len(history)} | ä¸Šä¸‹æ–‡æ¥æº: {session_context_source or 'å¸¸è§„'}")
        if history:
            print("ğŸ“œ è¾“å…¥å†å²è®°å½•:")
            for i, msg in enumerate(history):
                role_emoji = "ğŸ‘¤" if msg["role"] == "user" else "ğŸ¤–"
                print(f"  [{i+1}] {role_emoji} {msg['role']}")
                # é™åˆ¶å†…å®¹é•¿åº¦å¹¶è¿›è¡Œå®‰å…¨æ—¥å¿—å¤„ç†
                safe_preview = self._safe_for_logging(msg.get('content', ''), 80)
                print(f"      ğŸ“ {safe_preview}")
        else:
            print("ğŸ“œ è¾“å…¥å†å²è®°å½•ä¸ºç©º")

        # æ„å»º prompt
        messages = []
        
        # 1. æ·»åŠ  system_prompt
        if "system_prompt" in role_data:
            messages.append({"role": "system", "content": role_data["system_prompt"]})
        
        # 2. ä»…åœ¨éå¿«ç…§ä¼šè¯æ—¶æ·»åŠ è§’è‰²é¢„ç½® historyï¼ˆé¿å…é‡å¤ï¼‰
        if session_context_source != "snapshot" and "history" in role_data:
            messages.extend(role_data["history"])
            print(f"âœ… æ·»åŠ è§’è‰²é¢„ç½®å¯¹è¯: {len(role_data.get('history', []))} æ¡")
        elif session_context_source == "snapshot":
            print(f"â­ï¸ è·³è¿‡è§’è‰²é¢„ç½®å¯¹è¯ï¼ˆå¿«ç…§ä¼šè¯å·²åŒ…å«å®Œæ•´ä¸Šä¸‹æ–‡ï¼‰")
        
        # 3. æ·»åŠ å®é™…ä¼šè¯å†å²
        messages.extend(history)
        # æ³¨æ„ï¼šä¸å†é¢å¤–æ·»åŠ  user_inputï¼Œå› ä¸ºå®ƒå·²ç»åœ¨ history ä¸­äº†
        
        # ğŸ†• 4. å‰3è½®å¯¹è¯å¢å¼ºæŒ‡ä»¤é€»è¾‘
        user_turn_count = self._count_real_user_turns(history)
        if user_turn_count <= 3 and messages:
            last_user_msg_index = self._find_last_user_message_index(messages)
            if last_user_msg_index is not None:
                # å¢å¼ºæœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
                original_content = messages[last_user_msg_index]["content"]
                enhanced_content = self._enhance_user_message_with_instruction(
                    original_content, 
                    f"ç¬¬{user_turn_count}è½®å¯¹è¯"
                )
                messages[last_user_msg_index]["content"] = enhanced_content
                print(f"âœ… å·²ä¸ºç¬¬{user_turn_count}è½®å¯¹è¯æ·»åŠ å¢å¼ºæŒ‡ä»¤")
        elif user_turn_count > 3:
            print(f"â­ï¸ è·³è¿‡æŒ‡ä»¤å¢å¼ºï¼ˆå·²è¶…è¿‡3è½®ï¼‰: å½“å‰ç¬¬{user_turn_count}è½®")

        # æ‰“å°æ„å»ºçš„å®Œæ•´æ¶ˆæ¯åˆ—è¡¨
        print(f"ğŸ”§ æ„å»ºå®Œæ•´æ¶ˆæ¯åˆ—è¡¨ | æ€»æ¶ˆæ¯æ•°: {len(messages)}")
        print("ğŸ“‹ å®Œæ•´æ¶ˆæ¯åˆ—è¡¨:")
        for i, msg in enumerate(messages):
            role_emoji = {"system": "âš™ï¸", "user": "ğŸ‘¤", "assistant": "ğŸ¤–"}.get(msg["role"], "â“")
            print(f"  [{i+1}] {role_emoji} {msg['role']}")
            safe_preview = self._safe_for_logging(msg.get('content', ''), 80)
            print(f"      ğŸ“ {safe_preview}")
        
        print(f"ğŸ‘¤ å½“å‰ç”¨æˆ·è¾“å…¥: {self._safe_for_logging(user_input, 200)}")
        print("ğŸ§ " + "="*48)

        # æ¨¡æ‹Ÿè¶…æ—¶
        # ï¼ˆè¿™é‡Œåº”è¯¥åœ¨ GPTCaller å±‚åšçœŸæ­£çš„ async è¶…æ—¶æ§åˆ¶ï¼Œè¿™é‡Œå…ˆç®€åŒ–ï¼‰
        if random.random() < 0.01:
            raise TimeoutError("4004: ç”Ÿæˆè¶…æ—¶")

        # å¼€å§‹è®¡æ—¶ï¼šä»è°ƒç”¨GPT APIå¼€å§‹
        start = time.time()
        
        # æ”¶é›†å®Œæ•´å›å¤
        full_response = ""
        
        # è°ƒç”¨å¼‚æ­¥æµå¼ GPT API
        async for partial_reply in self.gpt.get_stream_response(messages, model_name=role_data.get("model"), timeout=timeout):
            full_response += partial_reply
            
            # å¦‚æœæä¾›äº†å›è°ƒå‡½æ•°ï¼Œé€æ­¥è°ƒç”¨å®ƒæ¥å¤„ç†éƒ¨åˆ†å›å¤
            if on_partial_reply:
                if callable(on_partial_reply):
                    # åŒæ­¥å›è°ƒ
                    on_partial_reply(partial_reply)
                else:
                    # å¼‚æ­¥å›è°ƒ
                    await on_partial_reply(partial_reply)

        # ç»“æŸæµå¼ç”Ÿæˆ
        print(f"ğŸ¤– AIç”Ÿæˆå›å¤å®Œæˆ | è€—æ—¶: {time.time() - start:.2f}ç§’ | æ€»å­—ç¬¦æ•°: {len(full_response)}")
        print("ğŸ¤–" + "="*48)
        
        return full_response

    def _count_real_user_turns(self, history):
        """
        ç»Ÿè®¡ä¼šè¯ä¸­çœŸå®ç”¨æˆ·å‘è¨€è½®æ¬¡
        åªç»Ÿè®¡ role == "user" çš„æ¶ˆæ¯æ•°é‡
        """
        user_turns = sum(1 for msg in history if msg.get("role") == "user")
        print(f"ğŸ“Š ç»Ÿè®¡ç”¨æˆ·å¯¹è¯è½®æ¬¡: {user_turns}")
        return user_turns
    
    def _find_last_user_message_index(self, messages):
        """
        æ‰¾åˆ°æ¶ˆæ¯åˆ—è¡¨ä¸­æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯çš„ç´¢å¼•
        ä»åå¾€å‰æŸ¥æ‰¾ï¼Œè¿”å›æœ€åä¸€æ¡ role == "user" çš„æ¶ˆæ¯ç´¢å¼•
        """
        for i in range(len(messages) - 1, -1, -1):
            if messages[i].get("role") == "user":
                print(f"ğŸ” æ‰¾åˆ°æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯ä½ç½®: index={i}")
                return i
        print("âš ï¸ æœªæ‰¾åˆ°ç”¨æˆ·æ¶ˆæ¯")
        return None
    
    def _enhance_user_message_with_instruction(self, original_content, user_context="å½“å‰å¯¹è¯"):
        """
        ä¸ºç”¨æˆ·æ¶ˆæ¯æ·»åŠ å‰3è½®å¢å¼ºæŒ‡ä»¤
        
        Args:
            original_content: åŸå§‹ç”¨æˆ·æ¶ˆæ¯å†…å®¹
            user_context: ç”¨æˆ·ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆç”¨äºæŒ‡ä»¤ä¸­çš„å ä½ç¬¦ï¼‰
        
        Returns:
            str: å¢å¼ºåçš„æ¶ˆæ¯å†…å®¹
        """
        enhanced_content = original_content + self.early_conversation_instruction.format(
            user_context=user_context
        )
        print(f"âœ¨ ç”¨æˆ·æ¶ˆæ¯å·²å¢å¼º | åŸé•¿åº¦: {len(original_content)} | å¢å¼ºåé•¿åº¦: {len(enhanced_content)}")
        return enhanced_content

    async def generate_reply_stream(self, role_data, history, user_input, timeout=60, session_context_source=None) -> AsyncGenerator[str, None]:
        """
        æµå¼ç”ŸæˆAIå›å¤ - è¿”å›å¼‚æ­¥ç”Ÿæˆå™¨ï¼Œç”¨äºTelegram Botçš„æµå¼æ›´æ–°
        
        Args:
            role_data: è§’è‰²é…ç½®æ•°æ®
            history: ä¼šè¯å†å²æ¶ˆæ¯
            user_input: å½“å‰ç”¨æˆ·è¾“å…¥
            timeout: è¶…æ—¶æ—¶é—´
            session_context_source: ä¼šè¯ä¸Šä¸‹æ–‡æ¥æºæ ‡è®°
            
        Yields:
            str: æ¯ä¸ªæµå¼å›å¤ç‰‡æ®µ
        """
        # æ‰“å°è¾“å…¥çš„å†å²è®°å½•
        print(f"ğŸ§  AIæµå¼ç”Ÿæˆå›å¤ | è¾“å…¥å†å²è®°å½•æ•°é‡: {len(history)} | ä¸Šä¸‹æ–‡æ¥æº: {session_context_source or 'å¸¸è§„'}")
        
        # æ„å»º promptï¼ˆå¤ç”¨ç›¸åŒé€»è¾‘ï¼‰
        messages = []
        
        # 1. æ·»åŠ  system_prompt
        if "system_prompt" in role_data:
            messages.append({"role": "system", "content": role_data["system_prompt"]})
        
        # 2. ä»…åœ¨éå¿«ç…§ä¼šè¯æ—¶æ·»åŠ è§’è‰²é¢„ç½® historyï¼ˆé¿å…é‡å¤ï¼‰
        if session_context_source != "snapshot" and "history" in role_data:
            messages.extend(role_data["history"])
            print(f"âœ… æ·»åŠ è§’è‰²é¢„ç½®å¯¹è¯: {len(role_data.get('history', []))} æ¡")
        elif session_context_source == "snapshot":
            print(f"â­ï¸ è·³è¿‡è§’è‰²é¢„ç½®å¯¹è¯ï¼ˆå¿«ç…§ä¼šè¯å·²åŒ…å«å®Œæ•´ä¸Šä¸‹æ–‡ï¼‰")
        
        # 3. æ·»åŠ å®é™…ä¼šè¯å†å²
        messages.extend(history)
        
        # ğŸ†• 4. å‰3è½®å¯¹è¯å¢å¼ºæŒ‡ä»¤é€»è¾‘ï¼ˆæµå¼ç‰ˆæœ¬ï¼‰
        user_turn_count = self._count_real_user_turns(history)
        if user_turn_count <= 3 and messages:
            last_user_msg_index = self._find_last_user_message_index(messages)
            if last_user_msg_index is not None:
                # å¢å¼ºæœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
                original_content = messages[last_user_msg_index]["content"]
                enhanced_content = self._enhance_user_message_with_instruction(
                    original_content, 
                    f"ç¬¬{user_turn_count}è½®å¯¹è¯"
                )
                messages[last_user_msg_index]["content"] = enhanced_content
                print(f"âœ… å·²ä¸ºç¬¬{user_turn_count}è½®å¯¹è¯æ·»åŠ å¢å¼ºæŒ‡ä»¤ï¼ˆæµå¼ï¼‰")
        elif user_turn_count > 3:
            print(f"â­ï¸ è·³è¿‡æŒ‡ä»¤å¢å¼ºï¼ˆå·²è¶…è¿‡3è½®ï¼‰: å½“å‰ç¬¬{user_turn_count}è½®")
        
        print(f"ğŸ”§ æ„å»ºå®Œæ•´æ¶ˆæ¯åˆ—è¡¨ | æ€»æ¶ˆæ¯æ•°: {len(messages)}")
        print("ğŸ§ " + "="*48)

        # æ¨¡æ‹Ÿè¶…æ—¶
        if random.random() < 0.01:
            raise TimeoutError("4004: ç”Ÿæˆè¶…æ—¶")

        # å¼€å§‹è®¡æ—¶
        start = time.time()
        
        # æµå¼ç”Ÿæˆå¹¶é€æ­¥è¿”å›
        chunk_count = 0
        total_chars = 0
        async for partial_reply in self.gpt.get_stream_response(messages, model_name=role_data.get("model"), timeout=timeout):
            chunk_count += 1
            total_chars += len(partial_reply)
            safe_chunk_preview = self._safe_for_logging(partial_reply, 50)
            print(f"ğŸ”„ æ”¶åˆ°chunk #{chunk_count}: {len(partial_reply)} å­—ç¬¦ | å†…å®¹é¢„è§ˆ: {safe_chunk_preview}...")
            yield partial_reply

        # ç»“æŸæµå¼ç”Ÿæˆ
        print(f"ğŸ¤– AIæµå¼ç”Ÿæˆå®Œæˆ | è€—æ—¶: {time.time() - start:.2f}ç§’ | æ€»chunkæ•°: {chunk_count} | æ€»å­—ç¬¦æ•°: {total_chars}")
        print("ğŸ¤–" + "="*48)


# âœ… å…¨å±€å”¯ä¸€å®ä¾‹ï¼ˆä¸´æ—¶å ä½ï¼Œå®é™…ä½¿ç”¨æ—¶åº”é€šè¿‡å®¹å™¨è·å–ï¼‰
# æ³¨æ„ï¼šè¿™ä¸ªå®ä¾‹åœ¨åˆå§‹åŒ–æ—¶ä¼šæŠ¥é”™ï¼Œå› ä¸ºæ²¡æœ‰æä¾› gpt_caller
# åœ¨åº”ç”¨å¯åŠ¨æ—¶ï¼Œåº”è¯¥é€šè¿‡å®¹å™¨åˆ›å»ºå¹¶æ›¿æ¢è¿™ä¸ªå®ä¾‹
ai_completion_port = None  # å°†åœ¨å®¹å™¨ä¸­åˆå§‹åŒ–
